---
layout: default
title: Damek Davis
---
# Damek Davis

[Papers](#publications) | [Research](ResearchStatementDamekDavis.pdf) | [CV](cv.pdf) | [X](https://twitter.com/damekdavis) | [Github](https://github.com/damek) | [Github (old)](https://github.com/COR-OPT) | [Scholar](https://scholar.google.com/citations?user=uGdPyZQAAAAJ&hl=en) | [Blog](random)  

I'm an Associate Professor in Wharton's Department of Statistics and Data Science. I was previously an Associate Professor at Cornell ORIE, an NSF Postdoctoral Fellow, and a PhD student in Math at UCLA under Wotao Yin (Alibaba) and Stefano Soatto (AWS AI). I was a long term visitor at the Simon's Institute in Fall 2017 (bridging discrete and continuous optimization) and Fall 2024 (LLM program). I am currently an associate editor at <a href="https://www.springer.com/journal/10107">Mathematical Programming</a> and <a href="https://www.springer.com/journal/10208">Foundations of Computational Mathematics</a>.

**Research Interests.** Optimization and machine learning. [See here](ResearchStatementDamekDavis.pdf).

**Teaching.** I teach theory and practice of optimization and machine learning. I sometimes write lecture notes, e.g., [Optimization in PyTorch](STAT-4830) and [Convex Analysis and First-Order Methods](https://damek.github.io/teaching/orie6300/ORIE6300Fall2023notes.pdf).

**Selected Awards.** I received a <a href="https://sloan.org/fellowships/">Sloan Research Fellowship in Mathematics</a>, an <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=2047637">NSF CAREER Award</a>, and the <a href="https://www.siam.org/prizes-recognition/activity-group-prizes/detail/siag-opt-best-paper-prize">SIAM Activity Group on Optimization Best Paper Prize</a>. 

**Selected Works.** Read more about my research [here](ResearchStatementDamekDavis.pdf). Together with collaborators, I 
- Identified [when/why spectral gradient methods help in deep learning](https://x.com/damekdavis/status/1996018764341805507?s=20).
- Developed exponential accelerations of <a href="https://x.com/damekdavis/status/1841596498204880924">gradient descent</a>, <a href="https://twitter.com/damekdavis/status/1596616542396944384">semismooth Newton</a>, and <a href="https://twitter.com/damekdavis/status/1682737261727866882?s=20">the subgradient method</a>. 
- Proved first guarantees for SGD on [weakly convex](https://arxiv.org/abs/1803.06523) and [tame](https://arxiv.org/abs/1804.07795) functions, which covers essentially all neural networks.
- Characterized the [asymptotic distribution of SGD in nonsmooth optimization](https://arxiv.org/abs/2301.06632).
- Developed the concept of a strict saddle point in nonsmooth optimization and showed [proximal methods](https://arxiv.org/abs/1912.07146) and [stochastic subgradient methods](https://arxiv.org/abs/2108.11832) avoid them.
- Developed [kernel methods](https://arxiv.org/abs/2505.08277) that efficiently learn sparse hierarchical functions.


**Students.** I've advised 5 PhD students. If you are a Penn student and wish to discuss advising/collaboration, send me a concise, informative email to set up a meeting. I am an active advisor--students who work best with me tend to have energy levels that match or exceed mine.


*Graduated PhD Students from Cornell*: 
- [Tao Jiang](https://taotolojiang.github.io/) → Meta (Postdoc) 
- [Liwei Jiang](https://liwei-jiang97.github.io/) →  Purdue (Assistant Professor)
- [Vasilis Charisopoulos](https://vchariso.gitlab.io/) → UW, Seattle (Assistant Professor)
- [Mateo Díaz](https://mateodd25.github.io/) → Johns Hopkins University (Assistant Professor)
- [Ben Grimmer](https://www.ams.jhu.edu/~grimmer/) → Johns Hopkins University (Assistant Professor)

**Blog.** I sporadically post notes [here](random).

Please use [my email](mailto:damek@wharton.upenn.edu) sparingly for correspondence related to consulting, research questions, teaching, or other professional inquiries. 

---

**You may not know that...** 
- I started [programming](https://x.com/damekdavis/status/1627803139830317056) in html in '98. I made a Pokémon website.
- I took up guitar In '99, played in bands, wrote and recorded music regularly into college. 
    - I played banjo from 2010-2012, mainly in clawhammer style.
    - I mostly play piano, now. I like to sing and sight read chord charts off of ultimate guitar.
- I worked throughout highschool, cleaning tables, bagging groceries, and making coffees. 
- In 2006, I applied to one college, UC Irvine. 
    - I could only afford one application fee and it was close to the beach.
    - I intended to study music and lived in the arts dorm. 
    - I took calculus my first semester and realized I loved math.
    - I took [algebra](https://www.math.uci.edu/~dwan/230-08) with [Daqing Wan](https://www.math.uci.edu/~dwan/) in 2009; we worked on a [commutative algebra problem](http://www.ams.org/journals/proc/2011-139-03/S0002-9939-2010-10620-2/).
- I went to grad school at UCLA for pure math in 2010. I loved [Algebra](https://link.springer.com/book/10.1007/978-1-4613-0041-0).
    - I learned ML was a thing in 2012 and took [learning from data](https://work.caltech.edu/telecourse) with set-theorist Bill Chen. 
    - I got excited about AI and joined [UCLA's vision lab](http://vision.ucla.edu/people.html) in 2012.
    - A year in, I saw optimization was everywhere. I just understood nothing about it.
    - I read [Nesterov's book](https://www.amazon.com/Introductory-Lectures-Convex-Optimization-Applied/dp/1402075537) in 2013. I tried to prove each theorem before reading the proof.
    - I took Wotao's course in F' 2013, and [solved an open problem](https://link.springer.com/chapter/10.1007/978-3-319-41589-5_4) he mentioned in class.
    - Without Wotao's encouragement, I would not have applied to faculty jobs in 2014.
- I became interested in writing in 2016.
    - Two books influenced me: [Clear and Simple as the Truth](https://www.amazon.com/Clear-Simple-Truth-Writing-Princeton/dp/0691602999) and [Style](https://www.amazon.com/Style-Lessons-Clarity-Grace-12th/dp/0134080416/).
        - The first helped me appreciate writing.
        - The second taught me how to structure text so a busy reader could appreciate it.
    - I think writing improves when you become more comfortable with rejection.
    - I decided to do more writing in public in 2025.
        - In the spring semester, I wrote [course notes for optimization in PyTorch](http://optimizationinpytorch.com).
        - In May, I started writing [notes](random) on what I've been thinking about.
        - In July-September, I [wrote about learning LLM engineering from scratch](https://x.com/damekdavis/status/1968079612044509347).
        - In October, I [wrote about the objective of reasoning with reinforcement learning](https://arxiv.org/abs/2510.13651).
- [Minerva](https://arxiv.org/abs/2206.14858) convinced me LLMs could eventually accelerate mathematics research. Since then,
    - I've spoken to LLMs more than anyone else I know.
        - I've use them for math, coding, writing, and even negotiating bills.
    - LLMs sort of helped me [almost formalize](https://github.com/damek/gd-lean) convergence of gradient descent in [Lean](https://lean-lang.org/). 
        - As of April 2025, no LLM could complete the proof.
    - I received a [grant](https://www.nsf.gov/awardsearch/showAward?AWD_ID=2523384&HistoricalAwards=false) that aims to make progress on the [Hadamard conjecture](https://en.wikipedia.org/wiki/Hadamard_matrix#Hadamard_conjecture) with RL tools.


## Publications

[Preprints](#preprints) | [Conference papers](#conference-papers) | [Journal papers](#journal-papers) | [Book chapters](#book-chapters) | [Expository](#expository) | [Reports](#technical-reports)

### Preprints

[What is the objective of reasoning with reinforcement learning?](https://arxiv.org/abs/2510.13651)
Damek Davis, Benjamin Recht
Manuscript (2025)

[Iteratively reweighted kernel machines efficiently learn sparse functions](https://arxiv.org/abs/2505.08277)
Libin Zhu, Damek Davis, Dmitriy Drusvyatskiy, Maryam Fazel
Manuscript (2025)

[Spectral norm bound for the product of random Fourier-Walsh matrices](https://arxiv.org/abs/2504.03148)
Libin Zhu, Damek Davis, Dmitriy Drusvyatskiy, Maryam Fazel
Manuscript (2025)


###  Conference papers 

[Gradient descent with adaptive stepsize converges (nearly) linearly under fourth-order growth](https://arxiv.org/abs/2409.19791)
Damek Davis, Dmitriy Drusvyatskiy, Liwei Jiang
Mathematical Programming (to appear)

[Online Covariance Estimation in Nonsmooth Stochastic Approximation](https://arxiv.org/abs/2502.05305)
Liwei Jiang, Abhishek Roy, Krishna Balasubramanian, Damek Davis, Dmitriy Drusvyatskiy, Sen Na
In Conference on Learning Theory (2025)

[Aiming towards the minimizers: fast convergence of SGD for overparametrized problems](https://arxiv.org/abs/2306.02601)
Chaoyue Liu, Dmitriy Drusvyatskiy, Mikhail Belkin, Damek Davis, Yi-An Ma
NeurIPS (2023) 

[A gradient sampling method with complexity guarantees for Lipschitz functions in high and low dimensions](https://arxiv.org/abs/2112.06969) 
Damek Davis, Dmitriy Drusvyatskiy, Yin Tat Lee, Swati Padmanabhan, Guanghao Ye
NeurIPS (2022) 
*Oral Presentation (top ~1%)*

[High probability guarantees for stochastic convex optimization](http://proceedings.mlr.press/v125/davis20a.html) 
Damek Davis, Dmitriy Drusvyatskiy
In Conference on Learning Theory (2020)

[Global Convergence of EM Algorithm for Mixtures of Two Component Linear Regression](http://proceedings.mlr.press/v99/kwon19a.html) 
Jeongyeol Kwon, Wei Qian, Constantine Caramanis, Yudong Chen, and Damek Davis
Conference on Learning Theory (2019)

[The Sound of APALM Clapping: Faster Nonsmooth Nonconvex Optimization with Stochastic Asynchronous PALM](https://papers.nips.cc/paper/6428-the-sound-of-apalm-clapping-faster-nonsmooth-nonconvex-optimization-with-stochastic-asynchronous-palm) 
Damek Davis, Brent Edmunds, Madeleine Udell
Neural Information Processing Systems (2016) | [report](https://arxiv.org/abs/1604.00526) 

[Multiview Feature Engineering and Learning](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Dong_Multi-View_Feature_Engineering_2015_CVPR_paper.pdf) 
Jingming Dong, Nikos Karianakis, Damek Davis, Joshua Hernandez, Jonathan Balzer and Stefano Soatto
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (2015)

[Asymmetric sparse kernel approximations for large-scale visual search.](http://www.vision.cs.ucla.edu/papers/davisBS14.pdf) 
Damek Davis, Jonathan Balzer, Stefano Soatto
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (2014)

### Journal papers 

[Active manifolds, stratifications, and convergence to local minima in nonsmooth optimization](https://arxiv.org/abs/2108.11832) 
Damek Davis, Dmitriy Drusvyatskiy, Liwei Jiang
Foundations of Computational Mathematics (to appear)

[Stochastic optimization over proximally smooth sets](https://arxiv.org/abs/2002.06309) 
Damek Davis, Dmitriy Drusvyatskiy, Zhan Shi
SIAM Journal on Optimization (to appear)

[Computational Microscopy beyond Perfect Lenses](https://arxiv.org/abs/2306.11283)
Xingyuan Lu, Minh Pham, Elisa Negrini, Damek Davis, Stanley J. Osher, Jianwei Miao
Physical Review E (to appear) 

[Global Optimality of the EM Algorithm for Mixtures of Two-Component Linear Regressions](https://ieeexplore.ieee.org/document/10614292)
Jeongyeol Kwon, Wei Qian, Constantine Caramanis, Yudong Chen, Damek Davis, Nhat Ho: 
IEEE Transactions on Information Theory (2024)

[Clustering a Mixture of Gaussians with Unknown Covariance](https://arxiv.org/abs/2110.01602) 
Damek Davis, Mateo Diaz, Kaizheng Wang
Bernoulli (to appear)

[Asymptotic normality and optimality in nonsmooth stochastic approximation](https://arxiv.org/abs/2301.06632) 
Damek Davis, Dmitriy Drusvyatskiy, Liwei Jiang
The Annals of Statistics (to appear)

[A nearly linearly convergent first-order method for nonsmooth functions with quadratic growth](https://arxiv.org/abs/2205.00064) 
Damek Davis, Liwei Jiang
Foundations of Computational Mathematics (to appear) | [code](https://github.com/COR-OPT/ntd.py) | [Twitter thread](https://twitter.com/damekdavis/status/1682389849167233027?s=20)

[Stochastic algorithms with geometric step decay converge linearly on sharp functions](https://arxiv.org/abs/1907.09547) 
Damek Davis, Dmitriy Drusvyatskiy, Vasileios Charisopoulos
Mathematical Programming (to appear) | [code](https://github.com/COR-OPT/GeomStepDecay) 

[A superlinearly convergent subgradient method for sharp semismooth problems](https://arxiv.org/abs/2201.04611) 
Vasileios Charisopoulos, Damek Davis
Mathematics of Operations Research (2023) | [code](https://github.com/COR-OPT/SuperPolyak.py) | [Twitter Thread](https://twitter.com/damekdavis/status/1596616542396944384)

[Escaping strict saddle points of the Moreau envelope in nonsmooth optimization](https://arxiv.org/abs/2106.09815) 
Damek Davis, Mateo Díaz, Dmitriy Drusvyatskiy
SIAM Journal on Optimization (2022)

[Variance reduction for root-finding problems](https://link.springer.com/article/10.1007/s10107-021-01758-4) 
Damek Davis
Mathematical Programming (to appear)

[Conservative and semismooth derivatives are equivalent for semialgebraic maps](https://arxiv.org/abs/2102.08484) 
Damek Davis, Dmitriy Drusvyatskiy
Set-Valued and Variational Analysis (to appear)

[From low probability to high confidence in stochastic convex optimization](https://arxiv.org/abs/1907.13307) 
Damek Davis, Dmitriy Drusvyatskiy, Lin Xiao, Junyu Zhang
Journal of Machine Learning Research (to appear)

[Proximal methods avoid active strict saddles of weakly convex functions](https://arxiv.org/abs/1912.07146) 
Damek Davis, Dmitriy Drusvyatskiy
Foundations of Computational Mathematics (2021)

[Low-rank matrix recovery with composite optimization: good conditioning and rapid convergence](https://arxiv.org/abs/1904.10020) 
Vasileios Charisopoulos, Yudong Chen, Damek Davis, Mateo Díaz, Lijun Ding, Dmitriy Drusvyatskiy
Foundations of Computational Mathematics (to appear) | [code](https://github.com/COR-OPT/CompOpt-LowRankMatrixRecovery)

[Composite optimization for robust rank one bilinear sensing](https://academic.oup.com/imaiai/advance-article-abstract/doi/10.1093/imaiai/iaaa027/5936039) 
Vasileios Charisopoulos, Damek Davis, Mateo Diaz, Dmitriy Drusvyatskiy
IMA Journal on Information and Inference (2020) | [code](https://github.com/COR-OPT/RobustBlindDeconv)

[Graphical Convergence of Subgradients in Nonconvex Optimization and Learning](https://arxiv.org/abs/1810.07590) 
Damek Davis, Dmitriy Drusvyatskiy
Mathematics of Operations Research (to appear)

[Proximally Guided Stochastic Subgradient Method for Nonsmooth, Nonconvex Problems.](https://arxiv.org/abs/1707.03505) 
Damek Davis, Benjamin Grimmer
SIAM Journal on Optimization (to appear) | [code](https://github.com/COR-OPT/PGSG/blob/master/Interactive-PGSG.ipynb)

[Trimmed Statistical Estimation via Variance Reduction](https://doi.org/10.1287/moor.2019.0992) 
Aleksandr Aravkin, Damek Davis
Mathematics of Operations Research (2019) | [video](https://www.youtube.com/watch?v=_HNQtTGDRNg)

[Stochastic subgradient method converges on tame functions.](https://arxiv.org/abs/1804.07795) 
Damek Davis, Dmitriy Drusvyatskiy, Sham Kakade, Jason D. Lee
Foundations of Computational Mathematics (to appear)

[The nonsmooth landscape of phase retrieval](https://academic.oup.com/imajna/article-abstract/40/4/2652/5684995) 
Damek Davis, Dmitriy Drusvyatskiy, Courtney Paquette
IMA Journal on Numerical Analysis (2018)

[Stochastic model-based minimization of weakly convex functions.](https://arxiv.org/abs/1803.06523) 
Damek Davis, Dmitriy Drusvyatskiy
SIAM Journal on Optimization (2019) | [blog](http://ads-institute.uw.edu//blog/2018/04/02/sgd-weaklyconvex/)
This is the combination of the two arXiv preprints  [arXiv:1802.02988](https://arxiv.org/abs/1802.02988)  and  [arXiv:1803.06523](https://arxiv.org/abs/1803.06523) 
Supplementary technical note:  [Complexity of finding near-stationary points of convex functions stochastically](https://arxiv.org/abs/1802.08556) 
Related report on nonsmooth nonconvex mirror descent  [Stochastic model-based minimization under high-order growth](http://www.optimization-online.org/DB_HTML/2018/07/6690.html)  (2018)
*INFORMS Optimization Society Young Researchers Prize (2019)*

[Subgradient methods for sharp weakly convex functions](https://link.springer.com/article/10.1007/s10957-018-1372-8) 
Damek Davis, Dmitriy Drusvyatskiy, Kellie J. MacPhee, Courtney Paquette
Journal of Optimization Theory and Applications (2018)

[Forward-Backward-Half Forward Algorithm for Solving Monotone Inclusions](https://doi.org/10.1137/17M1120099) 
Luis M. Briceño-Arias, Damek Davis
SIAM Journal on Optimization (2018)

[Convergence rate analysis of the forward-Douglas-Rachford splitting scheme.](https://doi.org/10.1137/140992291) 
Damek Davis
SIAM Journal on Optimization (2015)

[Convergence rate analysis of primal-dual splitting schemes](https://arxiv.org/abs/1408.4419) 
Damek Davis
SIAM Journal on Optimization (2015)

[Faster convergence rates of relaxed Peaceman-Rachford and ADMM under regularity assumptions](http://pubsonline.informs.org/doi/full/10.1287/moor.2016.0827) 
Damek Davis, Wotao Yin
Mathematics of Operations Research (2016)

[A Three-Operator Splitting Scheme and its Optimization Applications.](https://link.springer.com/article/10.1007/s11228-017-0421-z) 
Damek Davis, Wotao Yin
Set-Valued and Variational Analysis (2017) | [code](https://damek.github.io/ThreeOperators.html) | [slides](https://damek.github.io/ThreeOperators3-online.pdf) 

[Beating level-set methods for 5D seismic data interpolation: a primal-dual alternating approach](http://ieeexplore.ieee.org/document/7906537) 
Rajiv Kumar, Oscar López, Damek Davis, Aleksandr Y. Aravkin, Felix J. Herrmann
IEEE Transactions on Computational Imaging (2017)

[Tactical Scheduling for Precision Air Traffic Operations: Past Research and Current Problems](http://arc.aiaa.org/doi/full/10.2514/1.I010119) 
Douglas R. Isaacson, Alexander V. Sadovsky, Damek Davis
Journal of Aerospace Information Systems, April, Vol. 11, No. 4 : pp. 234-257

[Efficient computation of separation-compliant speed advisories for air traffic arriving in terminal airspace.](http://dynamicsystems.asmedigitalcollection.asme.org/article.aspx?articleid=1838670) 
Alexander V. Sadovsky, Damek Davis, Douglas R. Isaacson.
Journal of Dynamic Systems Measurement and Control 136(4), 041027 (2014)

[Separation-compliant, optimal routing and control of scheduled arrivals in a terminal airspace.](http://www.aviationsystemsdivision.arc.nasa.gov/publications/2013/Transportation_Research_Part_C_2013_Sadovsky.pdf) 
Alexander V. Sadovsky, Damek Davis, and Douglas R. Isaacson.
Transportation Research Part C: Emerging Technologies 37 (2013): 157-176

[Factorial and Noetherian Subrings of Power Series Rings.](http://www.ams.org/journals/proc/2011-139-03/S0002-9939-2010-10620-2/) 
Damek Davis, Daqing Wan
Proceedings of the American Mathematical Society 139 (2011), no. 3, 823-834

### Book chapters
[Convergence rate analysis of several splitting schemes](https://link.springer.com/chapter/10.1007/978-3-319-41589-5_4) 
Damek Davis, Wotao Yin
Splitting Methods in Communication and Imaging, Science and Engineering (2017) 
[video](https://www.youtube.com/watch?v=XDI9UbUkUz4) | [slides](https://damek.github.io/INFORMS_Presentation_Final.pdf) | [summary](https://damek.github.io/OStoday0515.pdf)
*Winner of the* [2014 INFORMS optimization society best student paper prize.](https://www.informs.org/Community/Optimization-Society/Optimization-Society-Prizes/Student-Paper-Prize/2014)

### Expository 

[A Short Course on Convex Analysis and First-Order Methods](https://damek.github.io/teaching/orie6300/ORIE6300Fall2023notes.pdf) 
Damek Davis
Manuscript (2023)

[Subgradient methods under weak convexity and tame geometry](research/papers/ViewsAndNews-28-1.pdf) 
Damek Davis, Dmitriy Drusvyatskiy
SIAG/OPT News and Views (2020)

[Convergence Rate Analysis of Several Splitting Schemes](https://damek.github.io/OStoday0515.pdf) 
Damek Davis
INFORMS OS Today (2015)

### Technical reports 
[A linearly convergent Gauss-Newton subgradient method for ill-conditioned problems](https://arxiv.org/abs/2212.13278)
Damek Davis, Tao Jiang
Technical report (2023) | [code](https://github.com/COR-OPT/GaussNewtonPolyak.py)

[Stochastic model-based minimization under high-order growth.](http://www.optimization-online.org/DB_HTML/2018/07/6690.html) 
Damek Davis, Dmitriy Drusvyatskiy, Kellie J. MacPhee
Technical Report (2018)

[An \(O(n\log(n))\) algorithm for projecting onto the ordered weighted \(\ell_1\) norm ball](http://www.optimization-online.org/DB_FILE/2015/03/4851.pdf) 
Damek Davis
UCLA CAM report 15-32 (2015) | [code](https://damek.github.io/OWLBall.html) 

[SMART: The Stochastic Monotone Aggregated Root-Finding Algorithm.](https://arxiv.org/abs/1601.00698) 
Damek Davis
Manuscript (2015) [ [slides](https://damek.github.io/Talks/SMART.pdf) ] [ [video](https://vimeo.com/156600995) ]

