\documentclass[12pt]{article}
%\usepackage{fullpage}
\usepackage[margin =1in]{geometry}
%\usepackage[inner=2cm,outer=1cm]{geometry}
\usepackage{amssymb,amsthm}
\usepackage{amsmath}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{cite}
\usepackage{enumitem}
\usepackage{color}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{tikzsymbols}
%\usepackage{url}

\numberwithin{equation}{section}
%%%%%%%%%% Math %%%%%%%%%%%%%%%

\newcommand{\ip}[1] {\langle #1 \rangle }
\newcommand{\norm}[1] {\left \| #1 \right \|}
\newcommand{\inclu}[0] {\ar@{^{(}->}}
\newcommand{\conn}{{\scriptstyle \#}}
\newcommand{\Aut}[1] {\text{Aut}_{#1}}
\newcommand{\Rank}{\text{Rank }}
\newcommand{\Ker}{\text{Ker }}
%\newcommand{\vec}{\text{Vec }}
\newcommand{\Dim}{\text{dim }}
\newcommand{\spann}{\text{span}}
\newcommand{\im}{\text{Im }}
\newcommand{\gph}{{\rm gph}\,}
\newcommand{\diag}{{\rm diag}}
\newcommand{\dist}{{\rm dist}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\trace}{\mathrm{Tr}}
\newcommand{\sign}{\mathrm{sign}}
\newcommand{\lip}{\mathrm{lip}}
\newcommand{\var}{\mathrm{var}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\pfail}{p_{\mathrm{fail}}}
\newcommand{\st}{\text{subject to:}}
\newcommand{\minimize}{\mathrm{minimize}}
\newcommand{\maximize}{\mathrm{maximize}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\SOC}{\mathrm{SOC}}
\newcommand{\sym}{\mathbb{S}}
\newcommand{\epi}{\mathrm{epi}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\range}{\mathrm{range}}
\newcommand{\cl}{\mathrm{cl}\,}
\newcommand{\bdry}{\mathrm{bdry}\,}
\newcommand{\cK}{\mathcal{K}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\val}{{\rm \texttt{val}}}
\newcommand{\aval}{{\rm \texttt{a-val}}}
\newcommand{\rank}{\mathrm{rank}}
\newcommand{\modelell}{l}
\newcommand{\conv}{\mathrm{conv}}


\graphicspath{ {./figures/} }

\newcommand{\abs}[1]{\left| #1 \right|}

%%%%%%%%% Convex Analysis %%%%%%%%%
\newcommand{\prox}{\mathrm{prox}}
\newcommand{\proj}{\mathrm{proj}}
\newcommand{\dom}{\mathrm{dom}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\ls}{\operatornamewithlimits{limsup}}
\newcommand{\Diag}{{\rm Diag}}
\newcommand{\argmax}{\text{argmax}}
\newcommand{\NN}{\mathbb{N}}

%%%% Probability %%%%%%%%%%%%%%%%%
\newcommand{\Prob}{\text{\textbf{Pr}}}
\newcommand{\Expect}{\mathbb{E}}
\newcommand{\matrx}[1]{\begin{bmatrix} #1 \end{bmatrix}}

%%%%%%%% Theorems %%%%%%%%%%%%%
\newtheorem{thm}{Theorem}[section]
\newtheorem{exercise}{Exercise}[section]
\newtheorem{definition}[thm]{Definition}
\newtheorem{proposition}[thm]{Proposition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{pa}{Part}
\newtheorem{subclaim}{Subclaim}
\newtheorem{assumption}{Assumption}
\renewcommand*{\theassumption}{\Alph{assumption}}

\newtheorem{remark}{Remark}
\newtheorem{notation}{Notation}
\newtheorem{example}{Example}[section]
\renewcommand*{\thepa}{\alph{pa}}

\theoremstyle{remark}
\newtheorem{claim}{Claim}



\usepackage{mathtools}
\DeclarePairedDelimiter{\dotp}{\langle}{\rangle}
\usepackage[boxruled]{algorithm2e}

\begin{document}
	
	\title{Homework Assignments for ORIE 6300: Mathematical Programming I}
	\author{Damek Davis\thanks{School of Operations Research and Information Engineering, Cornell University,
Ithaca, NY 14850, USA;
\texttt{people.orie.cornell.edu/dsd95/}.}}	
	

	
	
	\date{}
	\maketitle
	\newpage
\section{Homework 1}

\begin{enumerate}
\item Prove the following basic consequences of convexity:
\begin{enumerate}[noitemsep]
\item The set of optimal solutions to a convex program is convex.
\item Intersections of convex sets are convex. 
\item Cartesian products of convex sets are convex. 
\item If $\cX_1$ and $\cX_2$ are convex, then so is $\cX_1 + \cX_2 = \{ x_1 + x_2 \colon x_1 \in \cX_1, x_2 \in \cX_2\}.$
\item If $\cX\subseteq \RR^d $ is a convex set and $A$ is a matrix, then $\{Ax \colon x \in \cX\}$ is convex. 
\item If $\cY\subseteq \RR^m$ is a convex set and $A$ is a matrix, then $\{x \in \RR^d \colon Ax \in \cY\}$ is convex. 
\item The set $\{Ax \colon x \in \cX\}$ is  not necessarily closed, even when $\cX$ is closed.
\item A convex set $\cX \subseteq\RR^d$ has a convex closure.
\item Let $\cX$ be a closed convex set and let $x \in \cX$. 
Show that $\cN_{\cX}(x)$ is a closed convex cone, 
meaning $\cN_{\cX}(x)$ is 
closed 
	and 
convex 
		and 
	for all $v \in \cN_{\cX}(x)$ and $t \geq 0$, the inclusion $tv \in \cN_{\cX}(x)$ holds.
\end{enumerate}
\item Consider the $\ell_1$ ball:
$$
\cX := \left\{ x \in \RR^d \colon  \sum_{i=1}^d |x_i| \leq 1\right\}.
$$
\begin{enumerate}
\item Prove that $\cX$ is a \emph{polyhedron} (i.e., the intersection of finitely many linear inequalities, meaning $\cX = \{ x \in \RR^d \colon a_i^T x \leq b_i \text{ for $i = 1, \ldots, n$}\}$ for a set of vectors $a_i$ and scalars $b_i$). How many inequalities are needed to describe $\cX$ (how large is $n$)?
\item A \emph{lifting} of a polyhedron $\cP_1 \subseteq \RR^d$ is a description of the form $\cP_1 = \{Ax \colon x \in \cP_2\}$ where $\cP_2 \subseteq \RR^m$ is a polyhedron and $A \in \RR^{d \times m}$ is a matrix.  

Find a lifting of $\cX$ to $\RR^{2d}$, where the associate polyhedron in $\RR^{2d}$ is defined by at most $2d+1$ inequalities. 


\end{enumerate}
\item Calculate the normal cones of the following sets:
\begin{enumerate}[noitemsep]
\item $\cX = $ a subspace of $\RR^d$. 
\item $\cX = B_1(0)$ (closed unit ball in $\RR^d$)
\item $\cX = \RR_+^d = \{x \in \RR^d \colon x_i \geq 0 \text{ for } i = 1, \ldots, d\}$.
\item $\cX = \{x \in \RR^d \colon Ax = b\}$ where $ b\in \RR^m$ and $A \in \RR^{m \times d}$ is a matrix. 
\end{enumerate}
\item Let $f \colon \RR^d \rightarrow \RR \cup\{+ \infty\}$ be a convex function. Prove that any local minimum of $f$ is a global minimum.
\item ({\bf Weierstrass}) Let $f \colon \RR^d \rightarrow \RR \cup \{+\infty\}$ be a function that has a closed epigraph and bounded sublevel sets. Show that $f$ has a minimizer. (Hint: consider the epigraphical form from Section 2.2.1 of the course lecture notes.)
\item ({\bf The Rayleigh Quotient}; see Exercise 6 of Chapter 2.1 in Borwein and Lewis.)  
\begin{enumerate}
\item Let $f \colon \RR^d \backslash \{0\} \rightarrow \RR \cup \{+\infty\}$ be continuous, satisfying $f(\lambda x) = f(x)$ for all $\lambda >0$ in $\RR$ and nonzero $x$ in $\RR^d$. Prove $f$ has a minimizer. 
\item Given a symmetric matrix $A \in \RR^{d\times d}$, define a function $g(x) = x^T A x/\|x\|^2$ for nonzero $x \in \RR^d$. Prove that $g$ has a minimizer.
\item Calculate $\nabla g(x)$ for nonzero $x$.
\item Deduce that minimizers of $g$ must be eigenvectors, and calculate the minimum value.
\end{enumerate}
\end{enumerate}

\newpage

\section{Homework 2} 


Your homework partly relies on the following definition: 
\begin{definition}[Dual Cone]
Let $\cK \subseteq \RR^d$ be a cone. Then the \emph{dual cone} of $\cK$ is the set
$$
\cK^\ast := \{ s \in \RR^d \colon \dotp{x, s} \geq 0 \quad \forall x \in \cK\}.
$$
\end{definition}
Please complete the following exercises. 
\begin{enumerate}
\item Prove the following: 
\begin{enumerate}[noitemsep]
\item The closure of any cone must contain the origin. 
\item The intersection of two cones is a cone.
\item The Cartesian product of two cones is a cone.
\item If $\cK_1, \cK_2 \subseteq \RR^d$ are cones, then $\cK_1 + \cK_2$ is a cone. 
\item A cone $\cK \subseteq \RR^d$ is convex if and only if $\cK + \cK = \cK$.
\end{enumerate}
Suppose $A \in \RR^{m \times d}$ is a matrix. 
\begin{enumerate}[label=(\alph*),start=6,noitemsep]
\item If $\cK \subseteq \RR^d$ is a cone, then $\{Ax \colon x \in \cK\}$ is a cone in $\RR^m$.
\item If $\cK' \subseteq \RR^m$ is a cone, then $\{x \colon Ax \in \cK'\}$ is a cone in $\RR^d$.
\item Give an example of a closed convex cone $\cK \subseteq \RR^d$ and a matrix $A \in \RR^{m \times d}$ such that the set $\{Ax \colon x \in \cK\}$ is not closed. 
\end{enumerate}
\item 
\begin{enumerate}
\item Suppose $\cX$ is a closed convex set. Prove that 
$$
\cK_{\cX} = \{ (x, t) \colon t > 0 \text{ and }  x/t \in \cX\} 
$$
 is a convex cone. 
 \item If $\cX$ is bounded, show that $\overline \cK_{\cX} = \cK_\cX \cup \{(0, 0)\}.$
 \item Give an example of a closed convex set $\cX$ for which $\overline \cK_\cX \neq \cK_\cX \cup \{(0, 0)\}$.
 \end{enumerate}
 \item Let $\cK$ be a polyhedral cone.\footnote{The term polyhedral means the cone is defined by finitely many linear inequalities.} Prove that $\cK^\ast$ is also polyhedral. 
 \item Prove that each of the following cones $\cK$ are \emph{self-dual}, meaning $\cK = \cK^\ast$. 
 \begin{enumerate}
 \item $\RR_+^d$
 \item $\SOC(d+1)$ 
 \item $\mathbb{S}_+^{d\times d}$
 \end{enumerate}
  \item Let $\cX \subseteq \RR^d$ be a closed convex set. For any $x \in \cX$, define the \emph{proximal normal cone} 
 \begin{align*}
 \cN_{\cX}^P(x) = \left\{ v \in \RR^d \colon x = \proj_{\cX}(x + v) \right\}.
 \end{align*}
 Prove that 
 $
 \cN_{\cX}(x) = \cN_{\cX}^P(x).
 $
\end{enumerate}
\newpage

\section{Homework 3}


\begin{enumerate}[noitemsep]
\item {\bf (Normal Cone to A Cone.)} Let $\cK \subseteq \RR^m$ be a convex cone.  Prove that $$\cN_{\cK}(x) = -\cK^\ast \cap \{x\}^\perp \qquad \forall x \in \cK.$$
\item {\bf (A Compressive Sensing Problem.)}
Consider the following optimization problem 
\begin{align*}
\minimize&\;  \|x\|_1 \\
\st&\; Ax=b.
\end{align*}
(The symbol $\|x\|_1$ denotes the \emph{$\ell_1$ norm} on $\RR^d$, a particular member of the the family of $\ell_p$ norms defined as follows: for any $p \in [1, \infty)$, we define 
$$
\|x\|_p^p := \sum_{i=1}^d |x_i|^p \qquad \forall x \in \RR^d.
$$
If $ p = \infty$, we define $\|x\|_\infty := \max_{i = 1, \ldots, d} |x_i|$ for all $x \in \RR^d$.)
\begin{enumerate}[noitemsep]
\item\label{part:CS1} Write an equivalent linear programming formulation of this problem.
\item \label{part:CS2}Take the dual of the linear program from part~\ref{part:CS1}.
\item Prove that the linear program from part~\ref{part:CS2} is equivalent to the following problem
\begin{align*}
\maximize &\;  \dotp{y, b} \\
\st&\; \|A^Ty\|_\infty \leq 1.
\end{align*}
\end{enumerate}
\item {\bf (Failure Cases.)} 
\begin{enumerate}
\item Give an example of a linear program where $\val = + \infty$ and $\val^\ast = -\infty$.
\item Give an example of a conic program where $\val$ is finite but not attained. 
\item Give an example of a conic program where $\val = + \infty$, but $\val^\ast$ is finite. 
\item Give an example of a conic program where $\val, \val^\ast \in \RR$ and $\val \neq \val^\ast$.
\end{enumerate}
\item {\bf (Closed Functions.)}  Let $f : \RR^d \rightarrow [-\infty, + \infty]$ be an extended valued function.
\begin{enumerate}[noitemsep]
\item Prove there exists a unique function $\cl f : \RR^d \rightarrow [-\infty, +\infty]$, called the \emph{closure} of $f$, satisfying $$\epi(\cl f) = \overline{\epi(f)}.$$ Moreover, prove the closure satisfies the following limiting formula: 
\begin{equation}\label{eq:lsc}
\cl f(x) = \lim_{\varepsilon \rightarrow 0} \inf_{y \in B_{\varepsilon}(x)} f(y).
\end{equation}
\item Suppose $f$ is convex. Prove that $\cl f$ is convex. 
\end{enumerate}
{\bf Def.} An extended-valued function is \emph{closed} if $\epi(f)$ is closed.
\begin{enumerate}[resume, noitemsep]
\item Prove that $\cl f$ is closed. 
\item Prove that $\cl f(x) \leq f(x)$ for all $x \in \RR^d.$
\item  Suppose $f$ is continuous. Prove that $f$ closed. 
\item Suppose that $f$ is continuous at a point $x \in \RR^d$. Prove that $f(x) = \cl f(x)$. (In other words,
$$
f(x) =  \lim_{\varepsilon \rightarrow 0} \inf_{y \in B_{\varepsilon}(x)} f(y).)
$$
\item Suppose that for all $x \in \RR^d$, we have
$$
 f(x) = \lim_{\varepsilon \rightarrow 0} \inf_{y \in B_{\varepsilon}(x)} f(y).
$$
Prove that $f$ is closed. (Such functions are called lower semicontinuous.)
\item Give an example of a closed extended valued function such that $\dom(f) = \{x \colon f(x) < + \infty\}$ is open. 
\end{enumerate}
\item {\bf (Strong Duality.)} Let $A \in \RR^{m \times d}$, let $ c \in \RR^d$, and let $\cK \subseteq \RR^d$ be a closed convex cone. Consider the family of primal and dual conic problems, which both depend on a parameter  $b \in \RR^m$:
\begin{align}
\underbrace{
\left\{\begin{aligned}
\minimize &\; c^T x \\
\st & \; Ax = b  \\
& \; x \in \cK 
\end{aligned}\right\}}_{\cP(b)}  && && 
\underbrace{
\left\{
\begin{aligned}
\maximize &\; b^T x \\
\st & \; c - A^T y \in \cK^\ast  \\
&  
\end{aligned}
\right\}
}_{\cD(b)} 
\end{align}
Recall the value function $\val \colon \RR^m \rightarrow [-\infty, \infty]$
$$
\val(b) = \inf \{ c^T x \colon Ax = b, x \in \cK\} \qquad \forall b\in \RR^m,
$$
and the asymptotic value function $\aval \colon \RR^m \rightarrow [-\infty ,\infty]$
$$
\aval = \cl \val.
$$
\begin{enumerate}[noitemsep]
\item  Suppose there is a point $b\in \RR^m$ such that $\val(b) = \aval(b) \in \RR$. Prove that $\val(b') > -\infty$ for all $b' \in \RR^m$.
\item Give an example of a conic program and a vector $b$ such that the  $\val(b) = +\infty$ and $\aval(b) < +\infty$.
\item Suppose that $\val$ is continuous at a point $b \in \RR^m$. Prove that strong duality holds:
$$
\val(b) = \sup\{ b^T y \colon c - A^T y \in \cK^\ast\}.
$$
\item Prove that $\val$ is convex. Is $\aval$ convex?
\end{enumerate}
Consider the following basic property of convex functions:
\begin{thm}[Borwein and Lewis Theorem 4.1.3]
Let $ f \colon \RR^d \rightarrow (-\infty, + \infty]$ be a convex function. Then $f$ is continuous on the interior of its domain.\footnote{Recall that $\dom (f) = \{x \colon f(x) < + \infty\}$.}
\end{thm}
 Notice that the function $f$ in the above theorem never takes value $-\infty$.
\begin{enumerate}[resume]
\item We say that $\cP(b)$ is \emph{strongly feasible} if there exists an $\varepsilon > 0$ such that for all $b' \in B_{\varepsilon}(b)$ the perturbed problem $\cP(b')$ is feasible. 

Suppose that $\cP(b)$ is strongly feasible. Then show that strong duality holds: 
$$
\val(b) = \sup\{ b^T y \colon c - A^T y \in \cK^\ast\}.
$$
\item {\bf (Slater's Condition.)} If $\cP(b)$ has a feasible point $x$ lying in the interior of $\cK$ and if $\rank(A) = m$, prove that strong duality holds:
$$
\val(b) = \sup\{ b^T y \colon c - A^T y \in \cK^\ast\}.
$$
(Note that it is very common to assume $\rank(A) = m$ in the optimization literature, and we can assume this without loss of generality. Indeed, if $A$ is not full rank, we can simply row reduce the system and form a new problem with the row reduced matrix. Clearly, any solution to the new problem still solves the original one.) 
\end{enumerate}
\end{enumerate}

\newpage

\section{Homework 4}

\begin{enumerate}
\item {\bf (Extreme Points.)}
\begin{enumerate}
\item Give an example of a polyhedron with no extreme points. 
\item Prove that any nonempty polyhedron in standard form $P = \{x \colon Ax = b, x \geq 0\}$ has at least one extreme point.
\end{enumerate} 
\item {\bf (Polyhedral Functions.)}
We call a function $f : \RR^d \rightarrow (-\infty, \infty]$ \emph{polyhedral} if $\epi(f)$ is polyhedral. Prove that any polyhedral function $f$ admits the representation:
$$
f(x) = \max_{i = 1, \ldots, n} \{a_i^T x + b_i\} + \delta_{\cX}(x), \qquad \forall x \in \RR^d 
$$
where $n \geq 0$, $\cX \subseteq \RR^d$ is a polyhedral set, and for $i = 1, \ldots, n$, we have $a_i \in \RR^d$ and $b_i \in \RR$. (Hint: write $f(x) = \inf\{ t \colon (x, t) \in \epi(f)\}$.) Does the value function of a polyhedral program admit such a representation? Justify your answer.
\item {\bf (Strict Complementary Slackness.)} In this exercise, we examine the strict complementary slackness condition. To that end consider the following primal-dual pair of linear programs:  
 \begin{align}
\begin{aligned}
\minimize &\; c^T x \\
\st & \; Ax = b  \\
& \; x \in \RR_+ 
\end{aligned}&& && 
\begin{aligned}
\maximize &\; b^T x \\
\st & \; A^Ty + s - c = 0  \\
&  s \geq 0
\end{aligned}
\end{align}
Throughout this exercise, we suppose that optimal solutions exist. Consider the following condition.
~\\

\noindent {\bf Condition.} Suppose that there is some $j \in \{1, \ldots, d\}$ so that every optimal solution $x^\ast$ satisfies $x_j^\ast = 0$. 
~\\

In the next three parts, suppose the above condition holds. Under this condition, we will prove there is a dual optimal pair $(y, s)$ with $s_j > 0$.
\begin{enumerate}[noitemsep]
\item Consider the following linear program:
$$
\begin{aligned}
\minimize &\; {- x_j} \\
\st &\; Ax = b\\
&\; c^T x \leq \val\\
&\;  x \geq 0.
\end{aligned}
$$
Show that its dual is
$$
\begin{aligned}
\maximize &\; b^Ty - t\val \\
\st &\; A^Ty - tc + s = -e_j\\
&\;  s, t \geq 0,
\end{aligned}
$$
where $e_j$ denotes the $j$th standard basis vector. Prove that this dual has an optimal solution $(\bar y, \bar t, \bar s)$ and show that  $b^T \bar y = \bar t \val$. 
\item Suppose $\bar t > 0$ and let $y = \bar y/t$ and $s = (\bar s + e_j)/\bar t$. Prove that $s_j > 0$ (obvious) and $(y, s)$ solves the original dual problem.
\item Suppose that $\bar t = 0$. Find an optimal solution $(y, s)$ to the original dual problem with  $s_j > 0$. 
\end{enumerate}

Using the above results, we can construct a primal-dual pair satisfying the strict complementary slackness condition. To that end, define a subset of indices $J \subseteq \{1, \ldots, d\}$ by the following formula
$$
J := \{j  \colon \exists \text{ primal optimal $x$ with $x_j > 0$}\}.
$$
Using $J$, we will construct a sequence $(x^1, y^1), \ldots, (x^d, y^d)$ of primal-dual optimal pairs with the following properties: For each $j \in J$, we let $y^j$ be an arbitrary dual optimal solution and let $x^j$ be a primal optimal solution with $x^j_j > 0$. On the other hand, for each $j \notin J$, we let $x^j$ be an arbitrary primal optimal solution and let $y^j$ be a dual optimal optimal solution with $(c - A^Ty^j)_j > 0$ (exists by Parts 1-3). Given these primal-dual optimal pairs, define
$$
x^\ast := \frac{1}{d}\sum_{j=1}^d x^j \qquad \text{and} \qquad y^\ast := \frac{1}{d}\sum_{j = 1}^d y^j.
$$
\begin{enumerate}[resume]
\item {\bf Bonus.} Show that the pair $(x^\ast, y^\ast)$ is primal-dual optimal and in addition satisfies  strict complementary slackness, namely, 
$$
x_j^\ast > 0 \text{ if and only if } (c - A^Ty^\ast)_j = 0, \qquad \forall j
$$
\end{enumerate}

\item {(\bf A Closed Value Function.)} Prove that $\val : \RR^d \rightarrow (-\infty, +\infty]$ is closed if for every $\gamma, \tau \in \RR$, the set  
$$
\{x \colon c^Tx \leq \gamma, \|Ax\|\leq \tau, x \in \cK\}
$$
is bounded. Under this condition, prove that whenever $\val(b)$ is finite, strong duality holds ($\val = \val^\ast$) and there exists a primal optimal solution. 
\item Prove the following Lemma: 
\begin{lem}[Fr{\'e}chet Subgradients of Convex Functions]\label{eq:frechetconvex}
Let $f \colon \RR^d \rightarrow (-\infty, \infty]$ be a convex function. Then 
$$
\partial_F f(x) = \left\{v \colon f(y) \geq f(x) + \dotp{v, y - x}, \quad \forall y \in \RR^d \right\}, \qquad \forall x \in \dom(f). 
$$
Equivalently, $v \in \partial_F f(x)$ if and only if $f(y) - \dotp{v, y}$ is minimized at $x$. 
\end{lem} 

\item {\bf (Fermat's Rule.)} Prove the following Theorem
\begin{thm}[Fermat's Rule]\label{thm:Fermatrule}
Let $f : \RR^d \rightarrow (-\infty, \infty]$ be a proper function and suppose that $\bar x$ is a local minimizer of $f$. Then 
$$
0 \in \partial_F f(\bar x).
$$
If moreover $f$ is convex, the condition $0 \in \partial f(x)$ is both necessary and sufficient for $x$ to be a global minimum.
\end{thm}

\item {\bf (Mean Value Theorem.)} Suppose $f : \RR^d \rightarrow \RR$ is a closed convex function and let $x, y \in \RR^d$. Show that there exists $t \in [0, 1]$ such that 
$$
f(x) - f(y) \in \dotp{x - y, \partial f((1-t)x + ty)}
$$
(Hint: consider the convex function $t \mapsto f((1-t)x + ty) + t(f(x) - f(y)) $ on the compact interval $[0,1]$.)
\end{enumerate}
The next exercise relies on the following definition. 
\begin{definition}[Lipschitz Continuity] A function $ f: \RR^d \rightarrow \RR$ is called \emph{Lipschitz continuous} if 
$$
|f(x) - f(y)| \leq L \|x - y\|, \qquad \forall x, y \in \RR^d.
$$
for some $L > 0$. The constant $L$ is called a \emph{Lipschitz constant} of $f$.  
\end{definition}

\begin{enumerate}[resume]
\item {(\bf Lipschitz Continuity.)}  Let $f : \RR^d \rightarrow \RR$ be a closed convex function. Show that $f$ is Lipschitz continuous with Lipschitz constant $L$ if and only if for all $x \in \RR^d$, it holds 
$$
v \in \partial f(x) \implies \|v\| \leq L
$$
\end{enumerate}

\newpage
\section{Homework 5}

\begin{enumerate}
\item     Consider the simplex method applied to a standard form problem. Assume that the rows of the matrix $A$ are linearly independent. Prove or disprove the following.
        \begin{itemize}
            \item[(a)] A variable that just left the basis cannot reenter in the very next iteration (under any choice of pivoting rule).
            \item[(b)] A variable that just entered the basis cannot leave in the very next iteration (under any choice of pivoting rule).
            \item[(c)] If there is a nondegenerate optimal solution, then there exists a unique optimal basis.
            \item[(d)] If $x$ is an optimal solution, no more than $m$ of its components can be positive, where $m$ is the number of equality constraints.
        \end{itemize}

\item\Coffeecup Consider a polyhedron in standard form $\{x \colon Ax=b, x\geq 0\}$ and let $x,y$ be two different basic feasible solutions. If we are allowed to move from any basic feasible solution to an adjacent one in a single step, show that we can go from $x$ to $y$ in a finite number of steps.
\item {\bf Convex Hulls.} {\rm 
Let $\cX \subseteq \RR^d.$ We define the convex hull to be the smallest convex set containing $\cX$ and denote this set by $\conv(\cX)$. Here, the word ``smallest" means that whenever a convex set $\cY \subseteq \RR^d$ contains $\cX$, it must be the case that $\cY$ contains $\conv(\cX)$ as well. Prove that 
$$
\conv(\cX) = \left\{ x \in \RR^d \colon x = \sum_{i=1}^{n_x} \alpha_i x_i \text{ for some $n_x > 0$, $x_i \in \cX$, and $\alpha_i \in [0, 1]$ with } \sum_{i=1}^{n_x} \alpha_i = 1\right\}.
$$
}

\item {\rm {\bf Easy Subdifferential Facts.} 
\begin{enumerate}
\item Let $f \colon \RR^d \rightarrow (-\infty, \infty]$ be a closed, proper, convex function. Show that for all $x \in \dom(f)$, the set $\partial f(x)$ is closed and convex. 
\item Let $d = d_1 + \ldots + d_n$ for integers $d_i$ and let $f_i \colon \RR^{d_i} \rightarrow (-\infty, +\infty]$ be proper convex functions. Then 
$$
\partial (f_1 +\ldots +  f_n)(x_1, \ldots, x_n) = \partial f_1(x_1) \times \ldots \times  \partial f_n(x_n) \qquad  \forall x_i \in \dom(f_i) 
$$
\item Let $f \colon \RR^d \rightarrow (-\infty, \infty]$ be a closed, proper, convex function and let $\lambda > 0$. Then prove that the function $g = \lambda f$ is satisfies 
$$
\partial g(x) = \lambda \partial f(x), \qquad \forall x \in \dom(f). 
$$
\item  Let $f \colon \RR^d \rightarrow (-\infty, \infty]$ be a closed, proper, convex function and let $b \in \RR^d$. Then prove that the shifted function $g(\cdot) = f((\cdot) +b)$ satisfies
$$
\partial g(x) =  \partial f(x+b), \qquad \forall  x \in \dom(f) - \{b\}. 
$$
\end{enumerate}
}
\item {\rm Compute the subdifferentials of the following functions on $\RR^d$ (some are differentiable, others are easy applications of the chain rule/the easy from subdifferential facts in Exercise 4):
\begin{enumerate}[noitemsep]
\item {\bf $\ell_1$ norm.} $f(x) = \|x\|_1 = \sum_{i=1}^d |x_i|$.
%\item {\bf $\ell_\infty$ norm.} $f(x) = \|x\|_\infty = \max_{i \leq d} |x_i|$.
\item {\bf Hinge loss.} $f(x) = \max\{0, x\}$ (where $d=1$). 
\item {\bf Hybrid Norm.} $f(x) = \sqrt{1+x^2}$ (where $d=1$).
\item {\bf Logistic function.} $f(x) = \log(1 +\exp(x))$ (where $d=1$).% where $a_i \in \RR^{d}$ for $i = 1, \ldots, m$.
\item {\bf Indicator of $\ell_p$ ball.} \Coffeecup $f(x) = \delta_{\cX}(x)$ where for $p \in [1, \infty]$ and $\tau > 0$, we have $\cX = \{x \colon \|x\|_p \leq \tau\}$.
\item {\bf Max of coordinates.} \Coffeecup$f(x) = \max \{x_1, \ldots, x_d\}$.
\item {\bf Polyhedral Function.} $f(x) = \max_{i \leq m}\{ \dotp{a_i, x} + b_i\}$ where $a_1, \ldots, a_m \in \RR^d$ are vectors and $b_1, \ldots, b_m \in \RR$
\item {\bf Quadratic.} $f(x) = \frac{1}{2} \dotp{Ax, x}$ for some symmetric matrix $A \in \RR^{d\times d}$. 
\item {\bf Least Squares.} $f(x) = \frac{1}{2} \|Ax- b\|_2^2$ where $A \in \RR^{m \times d}$ and $b \in \RR^m$.
\item {\bf Least Absolute Deviations.} $f(x) = \|Ax- b\|_1$ where $A \in \RR^{m \times d}$ and $b \in \RR^m$.
%\item 
\end{enumerate}
}

\item {\bf Descent Directions} {\rm  
~
\begin{enumerate}
\item \Coffeecup Suppose that $f$ is Fr{\'e}chet differentiable on $\RR^d$ and that $\nabla f(x)$ is a continuous function of $x$. Show that for all $x \in \RR^d$ with $\nabla f(x) \neq 0$, there exists $\gamma > 0$ such that 
$$
f(x - \gamma \nabla f(x)) < f(x).
$$
({\bf Hint:} Consider the derivative of the one variable function $g(\gamma) = f(x - \gamma \nabla f(x))$.)
\item Consider a convex function $f(x, y) = a|x| + b|y|$ for scalars $a, b > 0$. Find a point $(x_0, y_0) \in \RR^2$, coefficients $a, b > 0$, and a subgradient $v \in \partial f(x, y)$ so that 
$$
f((x_0, y_0) - \gamma v) > f(x_0, y_0) \qquad \forall \gamma > 0.
$$
\item \Coffeecup\Coffeecup Let $f$ be a continuous convex function. Let $x \in \RR^d$ and suppose that $ 0 \notin \partial f(x)$. In this exercise, we will show that the minimal norm subgradient of $f$ at $x$ 
$$
v := \proj_{\partial f(x)}(0). 
$$
is a descent direction. 
\begin{enumerate}
\item  
Show that 
$$
\dotp{w, -v} \leq -\|v\|^2 \qquad \forall w \in \partial f(x). 
$$
\item Next, define the one variable continuous convex function $g(\gamma) = f(x - \gamma v)$. Show that 
$$
\eta \in \partial g(0) \implies \eta < - \|v\|^2.
$$
Can $0$ be a minimizer of $g$? 
\item Show that for all $\gamma < 0$, we have $g(\gamma) > g(0)$. 
\item Use parts (b) and (c) to show that for $g(\gamma) < g(0)$ for all sufficiently small $\gamma > 0$.  
\end{enumerate}
\end{enumerate}
}
\item {\rm 
Prove the following propositions. 
\begin{enumerate}
\item {\bf Clipped/Bundle Models.} Let $x \in \RR^d$ and suppose that $f_x$ is an $(\modelell, q)$ model of $f$ at $x$. Moreover, assume that $ g \colon\RR^d \rightarrow (-\infty, \infty]$ is closed, proper, convex, and dominated by $f$: $g(y) \leq f(y)$ for all $y \in \RR^d$. Then 
$$
\max\{f_x, g\}
$$
is an $(\modelell, q)$-model of $f$ at $x$.
\item {\bf Projected/Proximal Models.} Suppose that $f$ admits the decomposition
$$
f = g + h,
$$
where $g, h : \RR^d \rightarrow (-\infty, \infty]$ are closed, proper, convex functions.  
Let $x \in \RR^d$ and suppose that $g_x$ is an $(\modelell, q)$ model of $g$ at $x$.
Then 
$$
g_x + h
$$
is an $(\modelell, q)$-model of $f$ at $x$.
\item {\bf Max-Linear Models.} Suppose that $f$ admits the decomposition
$$
f = \max(f_1, \ldots, f_n),
$$
where for each $i$, the function $f_i: \RR^d \rightarrow (-\infty, \infty]$ is closed, proper, and convex.  
Let $x \in \RR^d$ and suppose for each $i$, the function $(f_i)_x$ is an $(\modelell, q)$ model of $f_i$ at $x$.
Then 
$$
 \max\{(f_1)_x, \ldots, (f_n)_x\}
$$
is an $(\modelell, q)$-model of $f$ at $x$.
\end{enumerate}
}
\item {\bf Clipping subproblem.}
{\rm 
\Coffeecup \Coffeecup Let $a, x \in \RR^d$, let {\rm $ \texttt{lb} \in \RR$}, let $\rho >0$, and let $b \in \RR$. Prove that the point
{\rm $$
x_+ = \argmin_{y \in \RR^d} \left\{ \max \{ \dotp{a,y} + b,  \texttt{lb}\} + \frac{\rho}{2}\|y - x\|^2 \right\}
$$}
satisfies 
$$
x_{+} = x - \texttt{clip}\left( \frac{\rho}{\|a\|^2} (\dotp{a, x} + b - \texttt{lb})\right) \frac{a}{\rho} \qquad \text{where} \qquad \texttt{clip}(t) = \max\{\min\{t,1\},0\}.% \label{eq:clip}
$$
({\bf Hint:} use first order optimality conditions.)
}
\end{enumerate}

\newpage
\section{Homework 6}

In this homework we study the core algorithmic subproblem in \emph{proximal algorithms.}
For motivation recall the \emph{proximal subgradient method} from lecture. 
This is perhaps the most common algorithm one encounters in first-order methods, so you should at least have a working knowledge of how to implement its steps, when possible.
In general it can be quite hard to implement these steps. 
Indeed, the subproblem includes as a special case the projection of a vector onto a convex set, a generally difficult task.
Still for a few useful functions we can implement these steps, even with simple closed form expressions.
\begin{enumerate}
\item Let $f \colon \RR^d \rightarrow (-\infty, \infty]$ be a closed, proper, convex function. Let $\gamma > 0$ and define the \emph{proximal operator} $\prox_{\gamma f}  \colon \RR^d \rightarrow \RR^d$:
$$
\prox_{\gamma f}(x) = \argmin_{y \in \RR^d} \left\{ f(y) + \frac{1}{2\gamma} \|y - x\|^2\right\}.
$$
\begin{enumerate}
\item Prove that for all $x \in \RR^d$, we have
$$
x_+ = \prox_{\gamma f} (x) \iff (x - x_+) \in \gamma \partial f(x_+)
$$
({\bf Hint:} use strong convexity.)
\item Prove that $x \in \RR^d$ is minimizes $f$ if and only if $x = \prox_{\gamma f}(x)$. 
\item {\bf (Minty's Theorem.)} Prove that 
$$
\range(I + \partial f) = \{ x + v \colon v \in \partial f(x) \} = \RR^d.
$$
({\bf Hint:} use part (a).)
\item \Coffeecup Prove that $\prox_{\gamma f}$ is $1$-Lipschitz, i.e., 
$$
\|\prox_{\gamma f}(x) - \prox_{\gamma f}(y)\| \leq \|x - y\|, \qquad \forall x, y \in \RR^d.
$$
({\bf Hint:} use strong convexity.)
\end{enumerate}
Notice the relation between proximal and projection operators: If $f(x) = \delta_{\cX}$ for a closed convex set $\cX$, then $\prox_{\gamma f} = \proj_{\cX}$ for all $\gamma > 0$. 
\item {\bf Calculus of Proximal Operators.} 
\begin{enumerate}
\item {\bf (Linear Perturbation.)} Suppose that $f \colon \RR^d \rightarrow (-\infty, \infty]$ is closed, proper, and convex, let $\gamma > 0$, and let $b, v \in \RR^d$. Define a function 
$$
g(x) = f(x + b) + v^T x, \qquad \forall x \in \RR^d
$$
Prove that 
$$
\prox_{\gamma g}(x) = \prox_{\gamma f}(x - \gamma v + b) - b, \qquad \forall x \in \RR^d
$$
({\bf Hint:} First try the cases where $b = 0$ or $v = 0$.)
\item {\bf (Separability.)} Let $d = d_1 + \ldots + d_n$ for integers $d_i$ and let $f_i \colon \RR^{d_i} \rightarrow (-\infty, +\infty]$ be proper convex functions. Let $\gamma > 0$ and for all $x = (x_1, \ldots, x_n) \in \RR^d$, define $f(x_1, \ldots, x_n) := \sum_{i=1}^n f(x_i)$. Prove that 
$$
\prox_{\gamma f}(x_1, \ldots, x_n) = (\prox_{\gamma f}(x_1), \ldots, \prox_{\gamma f_n}(x_n)), \qquad \forall x \in \RR^d.
$$
\item {\bf (Scalarization.)} \Coffeecup Let $f \colon \RR \rightarrow (-\infty, \infty]$ be a scalar function, let $\gamma > 0$, and let $a \in \RR^d\backslash \{0\}$. Define
$$
g(x) = f(a^Tx), \qquad \forall x \in \RR^d
$$
Prove that for all $x \in \RR^d$, we have
$$
\prox_{\gamma g}(x) = x - \rho a \qquad \text{where } \rho = \frac{1}{\|a\|^2} (a^T x - \prox_{(\gamma\|a\|^2) f}(a^T x)).
$$
({\bf Hint:} Be careful: the chain rule $\partial g(y) = a \partial f(a^T y)$ may not hold. Instead, use the inclusion $a \partial f(a^T y) \subseteq \partial g(y)$.)
\end{enumerate}
\item {\bf Proximal Operator Examples.} Compute the proximal operators of the following functions
\begin{enumerate}
\item $f(x) := \|x\|_1 = \sum_{i=1}^d |x_i|.$
\item $f(x) = \max\{0, x\}$ for a scalar variable $x \in \RR$.
\item $f(x) = \frac{1}{2}\dotp{Ax, x} - \dotp{b, x}$, where $b \in \RR^d$ and $A \in \RR^{d\times d}$ is a symmetric positive semidefinite matrix. 
\item $f(x) = \|x\|_2$.\\
({\bf Hint:} First compute the subdifferential of $f$, keeping in mind that $f$ is differentiable everywhere except the origin.) 
\item $f(x) = \delta_{\cX}$, where $\cX = \{ x \in \RR^d \colon x \geq 0\}$ is the nonnegative orthant.
\item $f(x) = \delta_{\cX}$, where $\cX = \{x \colon Ax = b\}$ is an affine space defined by matrix $A \in \RR^{m \times d}$ and $b \in \RR^m$.
\item $f(x) = \delta_{\cX}$, where $\cX = \{ x \colon \|x\|_\infty \leq 1\}$ \\
({\bf Hint:} You already computed $\partial f(x)$ on a previous homework assignment.) 
\end{enumerate}
\item {\bf(Projection onto $\mathbb{S}_+^{d\times d}$).} Recall that any symmetric matrix $A \in \mathbb{S}^{d\times d}$ (not necessarily positive semidefinite) has an eigenvalue decomposition 
$$
A = Q \Lambda Q^T\qquad \text{where } 
\left\{\begin{aligned}
 Q^TQ &= I \\
 \Lambda &= \diag(\lambda_1, \ldots, \lambda_d)\\
 &\hspace{14pt} \lambda_1 \geq \ldots \geq \lambda_d.
 \end{aligned}\right\}.
$$
For any such matrix, prove that 
$$
\proj_{\mathbb{S}_+^{d\times d}}(A) = Q\max \{ \Lambda, 0\} Q^T.
$$
({\bf Hint:} Verify the first order optimality conditions $A - \proj_{\mathbb{S}_+^{d\times d}}(A) \in \cN_{\mathbb{S}_+^{d\times d}}(\proj_{\mathbb{S}_+^{d\times d}}(A))$)
\end{enumerate}
Finally consider the following problem on sensitivity analysis for linear programs. 
\begin{enumerate}[resume]
\item 
\Coffeecup Consider the linear program $\min(c^Tx : x \geq 0, Ax = b)$. Let $B$ denote an optimal basis. Assume that the problem is generic in that each vertex has a unique basis for which it is the corresponding basic solution. Suppose now that you want to solve a parametric problem, i.e., a set of problems of the form $\min((c + \lambda d)^T x : x \geq 0, Ax = b)$, for each possible value of $\lambda \geq 0$. Assume that for any $\lambda \geq 0$ the problem has an optimal solution and that the basis $B$ is a solution for the problem when $\lambda = 0$.\\
\begin{enumerate}
\item Prove that the set of values of $\lambda$ for which basis $B$ is optimal forms an interval $[0, a_1]$. Explain how to compute $a_1$.\\
\item Show that there is a finite set $a_0 = 0 \leq a_1 \leq . . . \leq a_k$ and corresponding bases $B_i$ for $i = 0, . . . , k$ such that $B_0 = B$ and $B_i$ (for $i = 0, . . . , k$) is the optimal basis if and only if $\lambda \in [a_i, a_{i+1}]$, and $B_k$ is optimal if $\lambda \geq a_k$.
\end{enumerate}
\end{enumerate}

			\bibliographystyle{plain}
	\bibliography{bibliography}
\end{document}