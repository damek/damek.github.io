The stochastic subgradient method forms the algorithmic core of modern statistical and machine learning. We understand much about how it behaves for problems that are smooth or convex, but what guarantees does it have in the absence of smoothness and convexity? We prove that the stochastic subgradient method, on any semialgebraic locally Lipschitz function, produces limit points that are all first-order stationary. More generally, our result applies to any function with a Whitney stratifiable graph. In particular, this work endows the stochastic subgradient method, and its proximal extension, with rigorous convergence guarantees for a wide class of problems arising in data scienceâ€”including all popular deep learning architectures.