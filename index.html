<!DOCTYPE html>
<html>
    <head>
        <title>Damek Davis</title>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="bear-note-unique-identifier" content="5DFBF7F9-6146-42F4-ADC0-D832804587EE-598-000008997525642E">
        <meta name="created" content="2023-03-27T10:48:16-0700"/>
        <meta name="modified" content="2023-03-28T09:22:33-0700"/>
        <meta name="tags" content=""/>
        <meta name="last device" content="Damek’s MacBook Pro (2)"/>
    </head>
    <body>
        <div class="note-wrapper">
            <h1 id="Damek Davis">Damek Davis</h1>
<p>I am an associate professor of operations research at Cornell University. I received my Ph.D. in mathematics from the University of California, Los Angeles in 2015. My PhD advisors were <a href="https://wotaoyin.mathopt.com/">Wotao Yin</a> and <a href="https://web.cs.ucla.edu/~soatto/">Stefano Soatto</a>. </p>
<br>
<p>I am broadly interested in the mathematics of data science, particularly the interplay of optimization, signal processing, statistics, and machine learning.   I enjoy designing and analyzing learning algorithms based on optimization.  You can read more about my research in my <a href="#Research overview"">research overview.</a></p>
<br>
<p>My research has received several awards, including a <a href="https://sloan.org/fellowships/">Sloan Research Fellowship in Mathematics</a>, the <a href="https://www.informs.org/Recognizing-Excellence/Award-Recipients/Damek-Davis">INFORMS Optimization Society Young Researchers Prize</a>, an <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=2047637">NSF CAREER Award</a>, and the <a href="https://www.siam.org/prizes-recognition/activity-group-prizes/detail/siag-opt-best-paper-prize">SIAM Activity Group on Optimization Best Paper Prize</a>.</p>
<br>
<p><a href="cv.pdf">CV</a> | <a href="mailto:dsd95@cornell.edu">Email</a> | <a href="https://github.com/COR-OPT">Github</a> | <a href="https://scholar.google.com/citations?user=uGdPyZQAAAAJ&hl=en">Google Scholar</a>  </p>
<br>
<p><i>Note:</i> I am on sabbatical until July 2023.</p>
<br>
<h2 id="Students">Students</h2>
<p><b>Current PhD Students</b></p>
<p><a href="https://people.orie.cornell.edu/vc333/">Vasilis Charisopoulos</a> </p>
<p><a href="https://www.orie.cornell.edu/research/grad-students/liwei-jiang">Liwei Jiang</a>  </p>
<p><a href="https://www.orie.cornell.edu/research/grad-students/tao-jiang">Tao Jiang</a>  </p>
<br>
<p><b>Graduated PhD Students</b></p>
<p><a href="https://people.cam.cornell.edu/md825/">Mateo Díaz</a> </p>
<p>(Next position: Caltech CMS postdoc, then Asst Prof at JHU).</p>
<p><a href="https://people.orie.cornell.edu/bdg79/">Ben Grimmer</a> </p>
<p>(Next position: Asst Prof at JHU).</p>
<br>
<h2 id="Teaching">Teaching</h2>
<p><b>Lecture notes</b></p>
<p><a href="https://damek.github.io/teaching/orie6300/ORIE6300Fall2019notes.pdf">Optimization: Structure, Duality, Calculus, and Algorithms</a> </p>
<p>Draft of F’19 notes for my course  <a href="https://damek.github.io/orie6300.html">ORIE 6300</a> </p>
<p>(Last Update: 1/2020)</p>
<br>
<h1 id="Research overview">Research overview</h1>
<p>(My complete publication list can be viewed on my <a href="cv.pdf">CV</a> or <a href="https://scholar.google.com/citations?user=uGdPyZQAAAAJ&hl=en">Google Scholar</a> page.)</p>
<br>
<p>Learning algorithms work extremely well in practice, but we do not yet have a coherent mathematical foundation that explains when they provably work and how to improve their performance.  The challenge is that most learning algorithms rely on fitting highly nonlinear models via simple nonconvex optimization heuristics, and except for a few special cases, there is no guarantee they will find global optima. Despite this and NP-hardness, simple heuristics often do succeed, and over the last few years, I have studied why and when they do.</p>
<br>
<p>I spend a lot of time thinking about neural networks, and I am particularly curious whether we can provide provable convergence guarantees to standard training algorithms and whether these methods can be substantially improved. Deep networks fall outside the scope of classical optimization theory since they lead to problems that lack any conventionally useful notions of convexity or smoothness. To understand these methods, I think it is crucial to take the inherent <i>nonsmooth</i> structure of neural networks seriously. I study this structure and the associated algorithms by using and developing tools from several disciplines, including <a href="https://link.springer.com/book/10.1007/978-3-642-02431-3">nonsmooth/variational analysis</a>, <a href="https://epubs.siam.org/doi/10.1137/080722059">tame geometry</a>,  and <a href="https://www.cambridge.org/core/books/highdimensional-statistics/8A91ECEEC38F46DAB53E9FF8757C7A4E">high-dimensional statistics</a>. </p>
<br>
<h2 id="Algorithms for nonsmooth optimization in machine learning">Algorithms for nonsmooth optimization in machine learning</h2>
<p>While neural networks are nonsmooth, they are not pathological — they are built from just a few simple components, like polynomials, exponentials, logs, max’s, min’s, absolute values…. The best model of such non-pathological functions available in optimization is the so-called <i>tame class,</i> a class which appears in several of my papers and precludes <a href="https://en.wikipedia.org/wiki/Cantor_function">cantor function</a>-esque behavior. I have spent a lot of time trying to uncover notions of beneficial "partial" smoothness in tame optimization problems with the goal of exploiting this structure in algorithms. </p>
<br>
<p>While tame problems comprise virtually all tasks of interest, they appear to lack enough structure to endow simple iterative methods with global efficiency guarantees. A class with more structure, which can be viewed as a stepping stone between convex functions and general neural networks, is the so-called <i>weakly convex</i> class. These are functions which differ from convex functions by a simple quadratic. This class is deceptively simple, yet surprisingly broad and includes, for example, all C^2 smooth functions (on compact sets) and all compositions of Lipschitz convex functions with smooth mappings: h(c(x)). These losses appear throughout data science, and particularly prominently in low-rank matrix recovery problems (e.g., matrix completion and sensing). </p>
<br>
<p>My group has been working towards understanding the convergence of simple iterative methods, such as the stochastic subgradient method (SGD), on the tame and weakly convex problem classes. We have also been working towards designing methods which outperform SGD. </p>
<br>
<p>I will now briefly summarize some of the contributions of my group. For those interested, a brief technical introduction to some of my papers can be found in the expository note: </p>
<br>
<p><a href="https://damek.github.io/ViewsAndNews-28-1.pdf">Subgradient methods under weak convexity and tame geometry</a> </p>
<p>Damek Davis, Dmitriy Drusvyatskiy</p>
<p>SIAG/OPT Views and News (2020)</p>
<br>
<h3 id="An exponential speedup for “generic” tame problems">An exponential speedup for “generic” tame problems</h3>
<p>We developed the first first-order method that (locally) converges nearly linearly (i.e., exponentially fast) on “generic” tame problems. This is an exponential improvement on all prior first-order methods for this problem class.</p>
<br>
<p><a href="http://www.optimization-online.org/DB_HTML/2022/04/8898.html">A nearly linearly convergent first-order method for nonsmooth functions with quadratic growth</a> </p>
<p>Damek Davis, Liwei Jiang</p>
<p>Manuscript (2022)</p>
<br>
<h3 id="A superlinearly convergent method for “generic” tame equations">A superlinearly convergent method for “generic” tame equations</h3>
<p>We developed the first algorithm that (locally) converges nearly superlinearly (i.e., double exponentially fast) on “generic” tame equations. </p>
<br>
<p><a href="https://arxiv.org/abs/2201.04611">A superlinearly convergent subgradient method for sharp semismooth problems</a> </p>
<p>Vasileios Charisopoulos, Damek Davis</p>
<p>Mathematics of Operations Research (2023) | <a href="https://github.com/COR-OPT/SuperPolyak.py">code</a> | <a href="https://twitter.com/damekdavis/status/1596616542396944384">Twitter Thread</a></p>
<br>
<h3 id="Training guarantees for SGD on tame and weakly convex functions">Training guarantees for SGD on tame and weakly convex functions</h3>
<p>We showed that the stochastic subgradient method (e.g., backpropagation) converges to first order critical points on virtually any neural network.</p>
<br>
<p><a href="https://arxiv.org/abs/1804.07795">Stochastic subgradient method converges on tame functions</a> </p>
<p>Damek Davis, Dmitriy Drusvyatskiy, Sham Kakade, Jason D. Lee</p>
<p>Foundations of Computational Mathematics (2018) | <a href="https://damek.github.io/ICCOPT2019.pdf">Talk</a> </p>
<br>
<p>We proved the first sample/computational efficiency guarantees for the stochastic subgradient method on the weakly convex class. </p>
<br>
<p><a href="https://arxiv.org/abs/1803.06523">Stochastic model-based minimization of weakly convex functions</a> </p>
<p>Damek Davis, Dmitriy Drusvyatskiy</p>
<p>SIAM Journal on Optimization (2018) | <a href="http://ads-institute.uw.edu//blog/2018/04/02/sgd-weaklyconvex/">blog</a> </p>
<br>
<p><a href="https://arxiv.org/abs/1707.03505">Proximally Guided Stochastic Subgradient Method for Nonsmooth, Nonconvex Problems.</a> </p>
<p>Damek Davis, Benjamin Grimmer</p>
<p>SIAM Journal on Optimization (2018) [ <a href="https://github.com/COR-OPT/PGSG/blob/master/Interactive-PGSG.ipynb">code</a> ]</p>
<br>
<h3 id="Avoidable saddle points in nonsmooth optimization">Avoidable saddle points in nonsmooth optimization</h3>
<p>We developed the concept of an avoidable nonsmooth saddle point — non optimal points that algorithms may be drawn to. The proper formulation of this concept is well-known in C^2 smooth optimization, but was missing even for C^1 functions. We showed that both first-order and proximal methods do not converge to these points on “generic” tame problems:</p>
<br>
<p><a href="https://damek.github.io/OWOSNov2021.pdf">Talk: avoiding saddle points in nonsmooth optimization</a> </p>
<p>Updated (11/2021) | <a href="https://www.youtube.com/watch?v=6BOFWQhxYZE">video</a></p>
<br>
<p><a href="https://arxiv.org/abs/1912.07146">Proximal methods avoid active strict saddles of weakly convex functions</a> </p>
<p>Damek Davis, Dmitriy Drusvyatskiy</p>
<p>Foundations of Computational Mathematics (2021)</p>
<br>
<p><a href="https://arxiv.org/abs/2106.09815">Escaping strict saddle points of the Moreau envelope in nonsmooth optimization</a> </p>
<p>Damek Davis, Mateo Díaz, Dmitriy Drusvyatskiy</p>
<p>SIAM Journal on Optimization (2022)</p>
<br>
<p><a href="https://arxiv.org/abs/2108.11832">Active manifolds, stratifications, and convergence to local minima in nonsmooth optimization</a> </p>
<p>Damek Davis, Dmitriy Drusvyatskiy, Liwei Jiang</p>
<p>Manuscript (2022)</p>
<br>
<h3 id="Asymptotic normality of SGD in nonsmooth optimization">Asymptotic normality of SGD in nonsmooth optimization</h3>
<p>We characterized the asymptotic distribution of the error sequence in stochastic subgradient methods, proving it is asymptotically normal with “optimal covariance” on “generic” tame problems.</p>
<br>
<p><a href="https://arxiv.org/abs/2301.06632">Asymptotic normality and optimality in nonsmooth stochastic approximation</a> </p>
<p>Damek Davis, Dmitriy Drusvyatskiy, Liwei Jiang</p>
<p>Manuscript (2023)</p>
<br>
<h3 id="Low-rank matrix recovery: a stepping stone to neural networks">Low-rank matrix recovery: a stepping stone to neural networks</h3>
<p>We achieved the first sample complexity optimal and computationally optimal methods for several low-rank matrix recovery based on <i>nonsmooth</i> weakly convex formulations. Nonsmoothness was crucial to establishing these rates, since prior smooth formulations were known to suffer from “poor conditioning.”</p>
<br>
<p> </p>
<p><a href="https://academic.oup.com/imaiai/advance-article-abstract/doi/10.1093/imaiai/iaaa027/5936039">Composite optimization for robust rank one bilinear sensing</a> </p>
<p>Vasileios Charisopoulos, Damek Davis, Mateo Diaz, Dmitriy Drusvyatskiy</p>
<p>IMA Journal on Information and Inference (2020) [ <a href="https://github.com/COR-OPT/RobustBlindDeconv">code</a> ]</p>
<br>
<p><a href="https://academic.oup.com/imajna/article-abstract/40/4/2652/5684995">The nonsmooth landscape of phase retrieval</a> </p>
<p>Damek Davis, Dmitriy Drusvyatskiy, Courtney Paquette</p>
<p>IMA Journal on Numerical Analysis (2017) | <a href="https://damek.github.io/NonsmoothStatisticalAssumptions.pdf">Talk</a> </p>
<br>
<p>  </p>
<p><a href="https://arxiv.org/abs/1904.10020">Low-rank matrix recovery with composite optimization: good conditioning and rapid convergence</a> </p>
<p>Vasileios Charisopoulos, Yudong Chen, Damek Davis, Mateo Díaz, Lijun Ding, Dmitriy Drusvyatskiy</p>
<p>Foundations of Computational Mathematics (2019) | <a href="https://github.com/COR-OPT/CompOpt-LowRankMatrixRecovery">code</a> </p>
<br>
<h2 id="Other selected work">Other selected work </h2>
<p>Besides my work on nonconvex learning algorithms, I also have worked on clustering and convex optimization algorithms. </p>
<br>
<h3 id="Provable clustering methods and a potential statistical-to-computational gap">Provable clustering methods and a potential statistical-to-computational gap</h3>
<p>Clustering is a fundamental statistical problem of dividing a dataset into two or more groups.  Our work on this talk topic focuses on the classical setting wherein both clusters follow a Gaussian distribution with identical covariance, but distinct means. When the covariance matrix is known or “nearly spherical,” there are efficient algorithms to perform the clustering and achieve the “Bayes-optimal error rate.” When the covariance is unknown or poorly conditioned, there are no known algorithms that achieve the Bayes-optimal rate. </p>
<br>
<p>Our contribution to this topic is a surprising dichotomy for clustering with an unknown covariance matrix: on the one hand, the maximum likelihood estimator uncovers the correct clustering and achieves the Bayes-optimal error; on the other, we give evidence that no known algorithm can compute the maximum likelihood estimator unless one increases the number of samples by an order of magnitude. Thus, we conjecture that there is a <a href="https://arxiv.org/abs/1803.11132">statistical-to-computational gap</a> for this classical statistical problem.</p>
<br>
<p><a href="https://arxiv.org/abs/2110.01602">Clustering a Mixture of Gaussians with Unknown Covariance</a> </p>
<p>Damek Davis, Mateo Diaz, Kaizheng Wang</p>
<p>Manuscript (2021)</p>
<br>
<h3 id="Three-Operator Splitting and the complexity of splitting methods">Three-Operator Splitting and the complexity of splitting methods</h3>
<p>For my PhD thesis, I focused on a class of convex optimization algorithms, called operator-splitting methods. An operator splitting method is a technique for writing the solution of a “structured” convex optimization problems as the fixed-point of a well-behaved, nonlinear operator. Algorithmically, one then finds the fixed-point of the operator through, e.g., the classical <a href="https://en.wikipedia.org/wiki/Fixed-point_iteration">fixed-point iteration</a>. My best-known contributions to the topic include the (1) “Three-Operator-Splitting” method, which has been widely used throughout computational imaging, and (2) my work that established the convergence rates of several classical splitting methods, such as the Douglas-Rachford splitting method and Alternating Direction Method of Multipliers (ADMM).</p>
<br>
<p><a href="https://link.springer.com/article/10.1007/s11228-017-0421-z">A Three-Operator Splitting Scheme and its Optimization Applications</a> </p>
<p>Damek Davis, Wotao Yin</p>
<p>Set-Valued and Variational Analysis (2017)</p>
<br>
<p><a href="http://arxiv.org/abs/1406.4834">Convergence rate analysis of several splitting schemes</a> </p>
<p>Damek Davis, Wotao Yin</p>
<p>Splitting Methods in Communication and Imaging, Science and Engineering (2017)</p>

        </div>
        <script type="text/javascript">
            (function() {

    var doc_ols = document.getElementsByTagName("ol");

    for ( i=0; i<doc_ols.length; i++) {

        var ol_start = doc_ols[i].getAttribute("start") - 1;
        doc_ols[i].setAttribute("style", "counter-reset:ol_counter " + ol_start + ";");

    }

})();

        </script>
        <style>
            html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td,article,aside,canvas,details,embed,figure,figcaption,footer,header,hgroup,menu,nav,output,ruby,section,summary,time,mark,audio,video{margin:0;padding:0;border:0;font:inherit;font-size:100%;vertical-align:baseline}html{line-height:1}ol,ul{list-style:none}table{border-collapse:collapse;border-spacing:0}caption,th,td{text-align:left;font-weight:normal;vertical-align:middle}q,blockquote{quotes:none}q:before,q:after,blockquote:before,blockquote:after{content:"";content:none}a img{border:none}article,aside,details,figcaption,figure,footer,header,hgroup,main,menu,nav,section,summary{display:block}*{-moz-box-sizing:border-box;-webkit-box-sizing:border-box;box-sizing:border-box}html{font-size:87.5%;line-height:1.57143em}html{font-size:14px;line-height:1.6em;-webkit-text-size-adjust:100%}body{background:#fcfcfc;color:#545454;text-rendering:optimizeLegibility;font-family:"AvenirNext-Regular"}a{color:#de4c4f;text-decoration:none}h1{font-family:"AvenirNext-Medium";color:#333;font-size:1.6em;line-height:1.3em;margin-bottom:.78571em}h2{font-family:"AvenirNext-Medium";color:#333;font-size:1.3em;line-height:1em;margin-bottom:.62857em}h3{font-family:"AvenirNext-Medium";color:#333;font-size:1.15em;line-height:1em;margin-bottom:.47143em}p{margin-bottom:1.57143em;hyphens:auto}hr{height:1px;border:0;background-color:#dedede;margin:-1px auto 1.57143em auto}ul,ol{margin-bottom:.31429em}ul ul,ul ol,ol ul,ol ol{margin-bottom:0px}ol{counter-reset:ol_counter}ol li:before{content:counter(ol_counter) ".";counter-increment:ol_counter;color:#e06e73;text-align:right;display:inline-block;min-width:1em;margin-right:0.5em}b,strong{font-family:"AvenirNext-Bold"}i,em{font-family:"AvenirNext-Italic"}code{font-family:"Menlo-Regular"}.text-overflow-ellipsis{overflow:hidden;text-overflow:ellipsis;white-space:nowrap}.sf_code_string,.sf_code_selector,.sf_code_attr-name,.sf_code_char,.sf_code_builtin,.sf_code_inserted{color:#D33905}.sf_code_comment,.sf_code_prolog,.sf_code_doctype,.sf_code_cdata{color:#838383}.sf_code_number,.sf_code_boolean{color:#0E73A2}.sf_code_keyword,.sf_code_atrule,.sf_code_rule,.sf_code_attr-value,.sf_code_function,.sf_code_class-name,.sf_code_class,.sf_code_regex,.sf_code_important,.sf_code_variable,.sf_code_interpolation{color:#0E73A2}.sf_code_property,.sf_code_tag,.sf_code_constant,.sf_code_symbol,.sf_code_deleted{color:#1B00CE}.sf_code_macro,.sf_code_entity,.sf_code_operator,.sf_code_url{color:#920448}.note-wrapper{max-width:46em;margin:0px auto;padding:1.57143em 3.14286em}.note-wrapper.spotlight-preview{overflow-x:hidden}u{text-decoration:none;background-image:linear-gradient(to bottom, rgba(0,0,0,0) 50%,#e06e73 50%);background-repeat:repeat-x;background-size:2px 2px;background-position:0 1.05em}s{color:#878787}p{margin-bottom:0.1em}hr{margin-bottom:0.7em;margin-top:0.7em}ul li{text-indent:-0.35em}ul li:before{content:"•";color:#e06e73;display:inline-block;margin-right:0.3em}ul ul{margin-left:1.25714em}ol li{text-indent:-1.45em}ol ol{margin-left:1.25714em}blockquote{display:block;margin-left:-1em;padding-left:0.8em;border-left:0.2em solid #e06e73}.todo-list ul{margin-left:1.88571em}.todo-list li{text-indent:-1.75em}.todo-list li:before{content:"";display:static;margin-right:0px}.todo-checkbox{text-indent:-1.7em}.todo-checkbox svg{margin-right:0.3em;position:relative;top:0.2em}.todo-checkbox svg #check{display:none}.todo-checkbox.todo-checked #check{display:inline}.todo-checkbox.todo-checked+.todo-text{text-decoration:line-through;color:#878787}.code-inline{display:inline;background:white;border:solid 1px #dedede;padding:0.2em 0.5em;font-size:0.9em}.code-multiline{display:block;background:white;border:solid 1px #dedede;padding:0.7em 1em;font-size:0.9em;overflow-x:auto}.hashtag{display:inline-block;color:white;background:#b8bfc2;padding:0.0em 0.5em;border-radius:1em;text-indent:0}.hashtag a{color:#fff}.address a{color:#545454;background-image:linear-gradient(to bottom, rgba(0,0,0,0) 50%,#0da35e 50%);background-repeat:repeat-x;background-size:2px 2px;background-position:0 1.05em}.address svg{position:relative;top:0.2em;display:inline-block;margin-right:0.2em}.color-preview{display:inline-block;width:1em;height:1em;border:solid 1px rgba(0,0,0,0.3);border-radius:50%;margin-right:0.1em;position:relative;top:0.2em;white-space:nowrap}.color-code{margin-right:0.2em;font-family:"Menlo-Regular";font-size:0.9em}.color-hash{opacity:0.4}.ordered-list-number{color:#e06e73;text-align:right;display:inline-block;min-width:1em}.arrow svg{position:relative;top:0.08em;display:inline-block;margin-right:0.15em;margin-left:0.15em}.arrow svg #rod{stroke:#545454}.arrow svg #point{fill:#545454}mark{color:inherit;display:inline;padding:0.2em 0.5em;background-color:#fcffc0}img{max-width:100%;height:auto}

        </style>
    </body>
</html>
