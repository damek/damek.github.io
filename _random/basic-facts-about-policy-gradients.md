---
title: "Basic facts about policy gradients"
date: 2025-05-06
tags: [reinforcement-learning, policy-gradient]
description: "Basic math of policy gradients, actor critic, and proximal policy optimization"
---

### Problem setup

The goal of Reinforcement Learning (RL) is for an agent to learn "optimal" behavior through interaction with an environment. The agent's decisions are guided by a policy $\pi\_\theta(a\|s)$, parameterized by $\theta$. The objective is to find $\theta$ that maximizes 

$$J(\theta) = E_{\tau \sim \pi_\theta}[G_0],$$

Here, $G_0 = \sum_{k=0}^\infty \gamma^k r_{k+1}$ is the total discounted return for a trajectory $\tau=(s_0, a_0, r_1, s_1, \dots)$ generated by following policy $\pi_\theta$ from an initial state $s_0$ drawn from a distribution $p_0(s_0)$ with $\gamma \in [0,1]$ being the "discount factor." The distribution $p_0(s_0)$ specifies the probability of starting an episode in state $s_0$.

### Value functions and Bellman equations

To evaluate policies, we define value functions. These functions rely on the concept of the return from a generic time step $t$, $G\_t = \sum\_{k=0}^\infty \gamma^k r\_{t+k+1}$. Importantly, in this definition of $G\_t$, the discount $\gamma^k$ applies to the $k$-th reward *after* $r\_t$, meaning the "discount clock" effectively resets at time $t$.

1.  The *State-Value Function* $V^\pi(s) = E_\pi[G\_t \| s\_t=s]$ is the expected return from starting in state $s$ and following policy $\pi$.
2.  The *Action-Value Function* $Q^\pi(s,a) = E_\pi[G\_t \| s\_t=s, a\_t=a]$ is the expected return from starting in state $s$, taking action $a$, and then following policy $\pi$.

Because the environment and policy are stationary, the specific value of $t$ used in the definition does not change the resulting values $V^\pi(s)$ or $Q^\pi(s,a)$; they are functions of state (and action) only.

These value functions are related by the identity:

$$V^\pi(s) = \sum_a \pi(a|s) Q^\pi(s,a).$$

They satisfy the Bellman expectation equations, which express values recursively. Let $P(s'\|s,a)$ be the state transition probability and $R(s,a,s')$ be the expected immediate reward for $(s,a,s')$.

$$
\begin{aligned}
V^\pi(s) &= \sum_a \pi(a|s) \sum_{s'} P(s'|s,a) [R(s,a,s') + \gamma V^\pi(s')], \\
Q^\pi(s,a) &= \sum_{s'} P(s'|s,a) [R(s,a,s') + \gamma V^\pi(s')].
\end{aligned}
$$

These equations state that the value of a state (or state-action pair) under $\pi$ is the sum of the expected immediate reward and the discounted expected value of subsequent states encountered by following $\pi$.

### The policy gradient theorem

The Policy Gradient Theorem provides an expression for $\nabla_\theta J(\theta)$. It's useful because we can compute unbiased estimates of the gradient from samples of trajectories generated by $\pi_\theta$.

Let $P(\tau\|\theta)$ be the probability of trajectory $\tau$ under policy $\pi_\theta$.

$$J(\theta) = \sum_\tau P(\tau|\theta) G_0(\tau).$$

Then, $\nabla\_\theta J(\theta) = \sum_\tau \nabla_\theta P(\tau\|\theta) G\_0(\tau)$. Using the log-derivative trick, 

$$\nabla_\theta P(\tau|\theta) = P(\tau|\theta) \nabla_\theta \log P(\tau|\theta),$$

we get:

$$
\nabla_\theta J(\theta) = \sum_\tau P(\tau|\theta) (\nabla_\theta \log P(\tau|\theta)) G_0(\tau) = E_{\tau \sim \pi_\theta} [(\nabla_\theta \log P(\tau|\theta)) G_0(\tau)].
$$

The probability of a trajectory is $P(\tau\|\theta) = p\_0(s\_0) \prod\_{t=0}^\infty \pi\_\theta(a\_t\|s\_t) P(s\_{t+1}\|s\_t, a\_t)$. Thus, $\log P(\tau\|\theta) = \log p\_0(s\_0) + \sum\_{t=0}^\infty (\log \pi\_\theta(a\_t\|s\_t) + \log P(s\_{t+1}\|s\_t, a\_t))$. The gradient $\nabla\_\theta$ only affects terms involving $\pi\_\theta$:

$$\nabla_\theta \log P(\tau|\theta) = \sum_{t=0}^\infty \nabla_\theta \log \pi_\theta(a_t|s_t).$$

Substituting this back, we get:

$$
\nabla_\theta J(\theta) = E_{\tau \sim \pi_\theta} \left[ \left( \sum_{t=0}^\infty \nabla_\theta \log \pi_\theta(a_t|s_t) \right) G_0(\tau) \right]
$$

It can be shown that terms $\nabla_\theta \log \pi\_\theta(a\_k\|s\_k)$ for $k < t$ are uncorrelated with rewards $r\_{t'}$ for $t' \ge t$ given $s\_k, a\_k$. This "causality" argument allows us to replace $G\_0(\tau)$ with $G\_k(\tau)$ for the term $\nabla_\theta \log \pi\_\theta(a\_k\|s\_k)$, leading to a common form:

$$
\nabla_\theta J(\theta) = E_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^\infty \nabla_\theta \log \pi_\theta(a_t|s_t) G_t \right] \quad (*)
$$

In this expression, $G\_t = \sum\_{k=0}^\infty \gamma^k r\_{t+k+1}$ is the full discounted return experienced from state $s\_t$ onwards within the trajectory $\tau$. The term $\nabla_\theta \log \pi\_\theta(a\_t\|s\_t)$ is often called the "score function" for action $a\_t$ in state $s\_t$. The product of the score function and $G\_t$ determines the direction and magnitude of the update for actions taken along a trajectory. 

The gradient $E\_{\tau \sim \pi\_\theta} \left[ \sum\_{t=0}^\infty \nabla\_\theta \log \pi\_\theta(a\_t\|s\_t) G_t \right]$ is typically estimated using Monte Carlo sampling. For a single trajectory $\tau^{\text{sample}} \sim \pi\_\theta$, an unbiased estimate of this gradient is $\sum\_{t=0}^\infty (\nabla\_\theta \log \pi\_\theta(a\_t\|s\_t)) G\_t^{\text{sample}}$. A stochastic gradient ascent update then uses this estimate:

$$
\theta \leftarrow \theta + \alpha \left( \sum_{t=0}^\infty (\nabla_\theta \log \pi_\theta(a_t|s_t)) G_t^{\text{sample}} \right)
$$

where $G\_t^{\text{sample}}$ is the return from the sampled trajectory $\tau^{\text{sample}}$. In practice, the sum is truncated at a finite horizon $H$.

#### Connection to value functions

To connect this to value functions, we can utilize the definition of the action-value function, $Q^{\pi\_\theta}(s\_t,a\_t) = E\_{\pi\_\theta}[G\_t \| s\_t, a\_t]$. This states that $Q^{\pi\_\theta}(s\_t,a\_t)$ is the expected value of the random variable $G\_t$, conditioned on having been in state $s\_t$ and taken action $a\_t$, and subsequently following policy $\pi\_\theta$.

Using the law of total expectation ($E[X] = E\_Y[E[X\|Y]]$), we can rewrite the expectation in $(*)$:

$$
\begin{aligned}
\nabla_\theta J(\theta) &= E_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^\infty \nabla_\theta \log \pi_\theta(a_t|s_t) G_t \right] \\
&= \sum_{t=0}^\infty E_{\tau \sim \pi_\theta} \left[ \nabla_\theta \log \pi_\theta(a_t|s_t) G_t \right]  \\
&= \sum_{t=0}^\infty E_{(s_t,a_t) \text{ in } \tau \text{ at step } t} \left[ \nabla_\theta \log \pi_\theta(a_t|s_t) E[G_t | s_t, a_t, \pi_\theta] \right] \\
&= \sum_{t=0}^\infty E_{(s_t,a_t) \text{ in } \tau \text{ at step } t} \left[ \nabla_\theta \log \pi_\theta(a_t|s_t) Q^{\pi_\theta}(s_t,a_t) \right]
\end{aligned}
$$

Here, the expectation $E\_{(s\_t,a\_t) \text{ in } \tau \text{ at step } t}[\cdot]$ denotes averaging over all possible state-action pairs $(s\_t, a\_t)$ that can occur at time $t$ when trajectories are generated according to $s\_0 \sim p\_0(s\_0)$ and policy $\pi\_\theta$. This form, $\nabla_\theta J(\theta) = E\_{\tau \sim \pi\_\theta} \left[ \sum\_{t=0}^\infty \nabla_\theta \log \pi\_\theta(a\_t\|s\_t) Q^{\pi\_\theta}(s\_t,a\_t) \right]$, shows that the policy gradient depends on the Q-values of the actions taken.

### The Advantage Function: Improving Gradient Estimates

The variance of $G_t$ as an estimator for $Q^{\pi_\theta}(s_t,a_t)$ can be high. We can introduce a state-dependent baseline $b(s_t)$ into the policy gradient expression without changing its expectation:

$$\nabla_\theta J(\theta) = E_{\pi_\theta} [ \sum_t \nabla_\theta \log \pi_\theta(a_t|s_t) (Q^{\pi_\theta}(s_t,a_t) - b(s_t)) ].$$

This holds because 

$$
E_{a_t \sim \pi_\theta(\cdot|s_t)}[\nabla_\theta \log \pi_\theta(a_t|s_t) b(s_t)] = \sum_{a_t} \nabla_\theta \pi_\theta(a_t|s_t) b(s_t) = b(s_t) \nabla_\theta \sum_{a_t} \pi_\theta(a_t|s_t) = 0.
$$

When using samples $G_t$ in place of $Q^{\pi\_\theta}(s\_t,a\_t)$, the term in the gradient is $\nabla\_\theta \log \pi\_\theta(a\_t\|s\_t) (G\_t - b(s\_t))$. The variance of the scalar factor $(G\_t - b(s\_t))$, conditioned on $s\_t$, is minimized by choosing $b(s\_t) = E[G\_t\|s\_t] = V^{\pi\_\theta}(s\_t)$.


Thus, the optimal choice for $b(s_t)$ is $V^{\pi\_\theta}(s_t)$. This leads to using the *Advantage Function* 

$$
A^{\pi_\theta}(s_t, a_t) = Q^{\pi_\theta}(s_t, a_t) - V^{\pi_\theta}(s_t).
$$

The policy gradient estimate becomes 

$$
\sum_t \nabla_\theta \log \pi_\theta(a_t|s_t) (G_t - V_w(s_t)),
$$

where $V\_w(s\_t)$ is an estimate of $V^{\pi\_\theta}(s\_t)$, and $(G\_t - V\_w(s\_t))$ is an estimate of $A^{\pi\_\theta}(s\_t,a\_t)$.

### Actor-Critic Methods

Actor-Critic methods learn two distinct components:
1.  The *Actor*: A policy $\pi\_\theta(a\|s)$, parameterized by $\theta$, which determines how the agent behaves.
2.  The *Critic*: A state-value function approximator $V\_w(s)$, parameterized by $w$, which estimates $V^{\pi\_\theta}(s)$, the value of states under the current actor's policy.

These components are learned concurrently.

**Critic Learning:**
The critic $V\_w(s)$ (commonly a neural network) aims to approximate $V^{\pi\_\theta}(s)$. It is trained by minimizing a loss function that measures the discrepancy between its predictions and target values derived from experience. For TD(0) learning, after observing a transition $(s\_t, a\_t, r\_{t+1}, s\_{t+1})$, the target for $V\_w(s\_t)$ is 

$$
y_t = r_{t+1} + \gamma V_w(s_{t+1}).
$$

The critic's parameters $w$ are updated to minimize the squared error 

$$
(y_t - V_w(s_t))^2.
$$

The gradient of $\frac{1}{2}(y\_t - V\_w(s\_t))^2$ with respect to $w$ is $-(y\_t - V\_w(s\_t))\nabla\_w V\_w(s\_t)$, assuming $y\_t$ is treated as a fixed target during differentiation. This is only an approximation of the full gradient step method because the target $y\_t$ itself contains $V\_w(s\_{t+1})$, but its dependency on $w$ is ignored when computing the gradient for the update related to $V\_w(s\_t)$.

Given this gradient estimate, we can update the critics parameters using the stochastic gradient update:

$$
w \leftarrow w + \alpha_w (r_{t+1} + \gamma V_w(s_{t+1}) - V_w(s_t)) \nabla_w V_w(s_t)
$$

Here, the term 

$$
\delta_t = r_{t+1} + \gamma V_w(s_{t+1}) - V_w(s_t)
$$

is called the *TD error*. This TD error serves as an estimate of the advantage $A^{\pi\_\theta}(s\_t,a\_t)$. To see this relationship, recall $A^{\pi\_\theta}(s\_t,a\_t) = Q^{\pi\_\theta}(s\_t,a\_t) - V^{\pi\_\theta}(s\_t)$.
By definition, $Q^{\pi\_\theta}(s\_t,a\_t) = E_{\pi\_\theta}[r\_{t+1} + \gamma V^{\pi\_\theta}(s\_{t+1}) | s\_t, a\_t]$.
So, 

$$
A^{\pi_\theta}(s_t,a_t) = E_{\pi_\theta}[r_{t+1} + \gamma V^{\pi_\theta}(s_{t+1}) - V^{\pi_\theta}(s_t) | s_t, a_t].
$$

The TD error $\delta\_t = (r\_{t+1} + \gamma V\_w(s\_{t+1})) - V\_w(s\_t)$ is thus a sample-based estimate of the quantity inside this expectation, where $V\_w$ approximates $V^{\pi\_\theta}$, and the single observed $(r\_{t+1}, s\_{t+1})$ pair replaces the expectation over possible next states and rewards.

The TD error $\delta\_t$ is a biased estimate of $A^{\pi\_\theta}(s\_t,a\_t)$ if $V\_w \neq V^{\pi\_\theta}$. However, $\delta\_t$ can have lower variance as an advantage estimator compared to the estimate $(G\_t^{\text{sample}} - V\_w(s\_t))$. The return $G\_t^{\text{sample}} = r\_{t+1} + \gamma r\_{t+2} + \gamma^2 r\_{t+3} + \dots$ accumulates noise from many future random rewards. The TD error replaces the sum of all future discounted rewards beyond $r\_{t+1}$ (i.e., $\gamma G\_{t+1}^{\text{sample}}$) with a single bootstrapped estimate $\gamma V\_w(s\_{t+1})$. If $V\_w(s\_{t+1})$ is a reasonably stable (even if biased) estimate of $E[G\_{t+1}\|s\_{t+1}]$, then the variance of $r\_{t+1} + \gamma V\_w(s\_{t+1})$ can be much lower than the variance of $r\_{t+1} + \gamma G\_{t+1}^{\text{sample}}$. 

**Actor Update:**
The actor's parameters $\theta$ are updated using the TD error $\delta_t$ as the estimate of the advantage.

In an *online (per-step) setting*, the actor update is performed after each transition, using the $\delta_t$ computed for that step:

$$
\theta \leftarrow \theta + \alpha_\theta \nabla_\theta \log \pi_\theta(a_t|s_t) \delta_t
$$

This update adjusts the policy immediately based on the most recent experience.

In a *batch setting*, after collecting a batch of $N$ transitions and their corresponding TD errors $\{(s\_i, a\_i, \delta\_i)\}\_{i=1}^N$, the actor update sums these contributions:

$$
\theta \leftarrow \theta + \alpha_\theta \sum_{i=1}^{N} \nabla_\theta \log \pi_\theta(a_i|s_i) \delta_i
$$

This update adjusts the policy based on the aggregated experience in the batch. In both cases, the update aims to make actions that lead to a positive TD error (indicating a better-than-expected outcome relative to $V\_w(s\_t)$) more probable, and actions leading to a negative TD error less probable.


### Proximal Policy Optimization (PPO): Constraining Policy Updates for Improved Performance

Policy gradient methods update policy parameters $\theta$. When these updates are based on data collected under a previous policy $\pi\_{\theta\_{old}}$ (the "old" or behavior policy), and advantage estimates $\hat{A}\_t^{\theta\_{old}}$ are computed using $\pi\_{\theta\_{old}}$'s value function, large discrepancies between the new policy $\pi\_\theta$ and $\pi\_{\theta\_{old}}$ can make these advantage estimates inaccurate for evaluating $\pi\_\theta$. This can degrade performance. [Proximal Policy Optimization (PPO)](https://arxiv.org/abs/1707.06347) introduces mechanisms to obtain more reliable policy improvements by constraining how much the policy can change in each update step.

The primary objective is to find a new policy $\pi\_\theta$ such that its expected return $J(\theta)$ is greater than $J(\theta\_{old})$. A fundamental result from policy iteration theory (related to [Kakade & Langford, 2002](https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/KakadeLangford-icml2002.pdf)) provides an exact expression for this performance difference:

$$
J(\theta) - J(\theta_{old}) = E_{s \sim d^{\pi_\theta}} \left[ E_{a \sim \pi_\theta(\cdot|s)} [A^{\pi_{\theta_{old}}}(s,a)] \right]
$$

This identity states that the improvement in expected return is equal to the expected advantage of the actions taken by the new policy $\pi\_\theta$, where these advantages $A^{\pi\_{\theta\_{old}}}(s,a) = Q^{\pi\_{\theta\_{old}}}(s,a) - V^{\pi\_{\theta\_{old}}}(s)$ are calculated with respect to the *old* policy $\pi\_{\theta\_{old}}$. The outer expectation is over states $s$ visited according to the state visitation distribution $d^{\pi\_\theta}$ induced by the *new* policy $\pi\_\theta$.

Directly optimizing $J(\theta) - J(\theta_{old})$ using this expression is challenging because $d^{\pi\_\theta}$ depends on the parameters $\theta$ being optimized, making the expectation difficult to compute or differentiate.

To make optimization tractable, we form a local approximation. The first step in this approximation is to replace the expectation over states visited by the new policy, $s \sim d^{\pi\_\theta}$, with an expectation over states visited by the *old* policy, $s \sim d^{\pi\_{\theta\_{old}}}$. This substitution yields an approximation that is more accurate when $\pi\_\theta$ is close to $\pi\_{\theta\_{old}}$ (implying $d^{\pi\_\theta} \approx d^{\pi\_{\theta\_{old}}}$)

So, we approximate:

$$
E_{s \sim d^{\pi_\theta}} \left[ E_{a \sim \pi_\theta(\cdot|s)} [A^{\pi_{\theta_{old}}}(s,a)] \right] \approx E_{s \sim d^{\pi_{\theta_{old}}}} \left[ E_{a \sim \pi_\theta(\cdot|s)} [A^{\pi_{\theta_{old}}}(s,a)] \right]
$$

Now, the inner expectation $E_{a \sim \pi_\theta(\cdot\|s)} [A^{\pi_{\theta_{old}}}(s,a)]$ is still with respect to actions from the new policy $\pi\_\theta$. Since we are working with data (trajectories) generated by $\pi\_{\theta\_{old}}$, we use importance sampling to rewrite this inner expectation in terms of actions $a \sim \pi\_{\theta\_{old}}(\cdot\|s)$:

$$
\begin{aligned}
E_{a \sim \pi_\theta(\cdot|s)} [A^{\pi_{\theta_{old}}}(s,a)] &= \sum_a \pi_\theta(a|s) A^{\pi_{\theta_{old}}}(s,a) \\
&= \sum_a \pi_{\theta_{old}}(a|s) \frac{\pi_\theta(a|s)}{\pi_{\theta_{old}}(a|s)} A^{\pi_{\theta_{old}}}(s,a)
\end{aligned}
$$

Substituting this back into the approximation for $J(\theta) - J(\theta\_{old})$, we obtain a surrogate objective for the performance improvement (ignoring the constant $J(\theta\_{old})$ for maximization purposes):

$$
L_{\theta_{old}}^{IS}(\theta) = E_{s \sim d^{\pi_{\theta_{old}}}, a \sim \pi_{\theta_{old}}(\cdot|s)} \left[\frac{\pi_\theta(a|s)}{\pi_{\theta_{old}}(a|s)} A^{\pi_{\theta_{old}}}(s,a)\right]
$$

Here, $\rho\_t(\theta) = \frac{\pi\_\theta(a\_t\|s\_t)}{\pi\_{\theta\_{old}}(a\_t\|s\_t)}$ is the *importance sampling ratio* for a state-action pair $(s\_t, a\_t)$ from data collected under $\pi\_{\theta\_{old}}$. This ratio re-weights the advantage $A^{\pi\_{\theta\_{old}}}(s\_t,a\_t)$ to account for the relative probability of taking action $a\_t$ under $\pi\_\theta$ versus $\pi\_{\theta\_{old}}$. The objective $L\_{\theta\_{old}}^{IS}(\theta)$ provides an estimate of policy performance improvement, which is a first-order approximation accurate when $\pi\_\theta$ is close to $\pi\_{\theta\_{old}}$. Maximizing $L\_{\theta\_{old}}^{IS}(\theta)$ aims to increase the probability of actions that had high advantages under $\pi\_{\theta\_{old}}$, correctly weighted for the policy change.

This expression for $L\_{\theta_{old}}^{IS}(\theta)$ is an expectation over state-action pairs $(s,a)$. The sampling process defined by the expectation is as follows: first, a state $s$ is drawn according to $d^{\pi\_{\theta_{old}}}(s)$, the distribution representing the frequency of visiting state $s$ under policy $\pi\_{\theta\_{old}}$. Then, given $s$, an action $a$ is drawn according to $\pi\_{\theta\_{old}}(a\|s)$. The term $d^{\pi\_{\theta\_{old}}}(s) \pi\_{\theta\_{old}}(a\|s)$ thus represents the joint probability (or probability density) of the pair $(s,a)$ under this process. The expectation can be written explicitly as:

$$
L_{\theta_{old}}^{IS}(\theta) = \sum_s \sum_a \left( d^{\pi_{\theta_{old}}}(s) \pi_{\theta_{old}}(a|s) \right) \left[\frac{\pi_\theta(a|s)}{\pi_{\theta_{old}}(a|s)} A^{\pi_{\theta_{old}}}(s,a)\right]
$$

In practice, this expectation is estimated from data. We execute $\pi\_{\theta\_{old}}$ in the environment to generate a batch of trajectories. Each time step $t$ within these trajectories yields a specific state-action pair $(s\_t, a\_t)$ that was encountered. This collection of observed $(s\_t, a\_t)$ pairs from the batch forms an *empirical distribution* over state-action pairs.
Under suitable conditions (e.g., ergodicity of the Markov chain induced by $\pi\_{\theta\_{old}}$ and a sufficiently large batch of data), this empirical distribution approximates the true underlying joint distribution $P(s,a\|\pi\_{\theta\_{old}}) = d^{\pi\_{\theta\_{old}}}(s) \pi\_{\theta\_{old}}(a\|s)$. Consequently, a Monte Carlo estimate of $L\_{\theta\_{old}}^{IS}(\theta)$ is formed by averaging the term $\left[\frac{\pi\_\theta(a\_t\|s\_t)}{\pi\_{\theta\_{old}}(a\_t\|s\_t)} \hat{A}^{\pi\_{\theta\_{old}}}(s\_t,a\_t)\right]$ over all $(s\_t, a\_t)$ pairs in the collected batch (i.e., samples from the empirical distribution), using an estimated advantage $\hat{A}^{\pi\_{\theta\_{old}}}(s\_t,a\_t)$ for each.

One could in priciple optimize $L\_{\theta\_{old}}^{IS}(\theta)$ directly using the above estimate strategy. However, if $\rho\_t(\theta)$ becomes very large or very small (i.e., $\pi\_\theta$ significantly differs from $\pi\_{\theta\_{old}}$), the variance of the gradient of $L\_{\theta\_{old}}^{IS}(\theta)$ (when estimated from samples) can be large. PPO is an attempt to address this by further modifying this surrogate objective. 

The PPO clipped objective $L^{CLIP}(\theta)$ builds upon this same principle of empirical estimation. It is an average where, for each observed time step $t$ in the collected batch, the core term $\rho_t(\theta) \hat{A}_t^{\theta_{old}}$ is first subject to the clipping mechanism before being included in the average:

$$
L^{CLIP}(\theta) = \hat{E}_t \left[ \min\left( \rho_t(\theta) \hat{A}_t^{\theta_{old}} , \operatorname{clip}(\rho_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t^{\theta_{old}} \right) \right]
$$

Here:
*   The subscript $t$ in $\hat{E}\_t$ explicitly indicates that this is an empirical average over all time steps $t$ present in the collected batch of data. If the batch contains $M$ total time steps across all trajectories, then $\hat{E}\_t[X\_t]$ means $\frac{1}{M} \sum\_{j=1}^{M} X\_j$, where $X\_j$ is the quantity (e.g., $\min(\dots)$) evaluated for the $j$-th $(s\_t, a\_t, \hat{A}\_t^{\theta\_{old}})$ tuple in the batch.
*   $\rho\_t(\theta) = \frac{\pi\_\theta(a\_t\|s\_t)}{\pi\_{\theta\_{old}}(a\_t\|s\_t)}$ is calculated for the specific $s\_t, a\_t$ of that $j$-th time step sample.
*   $\hat{A}\_t^{\theta\_{old}}$ is the advantage estimate calculated for that specific $s\_t, a\_t$ sample.



The purpose of the $\min$ and $\operatorname{clip}$ operations is to form a more conservative (pessimistic) objective compared to $L\_{\theta\_{old}}^{IS}(\theta)$, effectively creating a lower bound on the expected improvement when the policy change is small.
*   **Case 1: $\hat{A}\_t^{\theta\_{old}} > 0$ (Advantageous action):**
    The unclipped term $\rho\_t(\theta) \hat{A}\_t^{\theta\_{old}}$ encourages increasing $\rho\_t(\theta)$. The clipped term becomes $(1+\epsilon)\hat{A}\_t^{\theta\_{old}}$ if $\rho\_t(\theta) > 1+\epsilon$. The $\min$ operator then takes $\min(\rho\_t(\theta) \hat{A}\_t^{\theta\_{old}}, (1+\epsilon)\hat{A}\_t^{\theta\_{old}})$. This prevents the objective from growing excessively if $\pi\_\theta$ makes an advantageous action much more likely (i.e., $\rho\_t(\theta)$ becomes very large), capping the contribution.
*   **Case 2: $\hat{A}\_t^{\theta\_{old}} < 0$ (Disadvantageous action):**
    The unclipped term $\rho\_t(\theta) \hat{A}\_t^{\theta\_{old}}$ encourages decreasing $\rho\_t(\theta)$. The clipped term becomes $(1-\epsilon)\hat{A}\_t^{\theta\_{old}}$ if $\rho\_t(\theta) < 1-\epsilon$. Since $\hat{A}\_t^{\theta\_{old}}$ is negative, a larger product (less negative or more positive) means less "penalty." The $\min$ operator selects the smaller (more negative, i.e., more penalizing) of the two terms: $\min(\rho\_t(\theta)\hat{A}\_t^{\theta\_{old}}, \text{clip}(\rho\_t(\theta),1-\epsilon,1+\epsilon)\hat{A}\_t^{\theta\_{old}})$. This ensures that if $\pi\_\theta$ attempts to drastically reduce the probability of a disadvantageous action (making $\rho\_t(\theta)$ very small), the objective still reflects a substantial penalty, preventing the policy from escaping this penalty too easily by making the ratio $\rho\_t(\theta)$ smaller than $1-\epsilon$.

Thus, the PPO objective $L^{CLIP}(\theta)$ is a practical sample-based objective that incrementally improves the policy. It processes each time step $(s\_t, a\_t)$ from trajectories run under $\pi\_{\theta\_{old}}$, calculates the importance-weighted advantage, applies the clipping rule to this term, and then averages these clipped terms over the entire batch of collected experiences. This averaging provides a Monte Carlo estimate of the expectation of the clipped quantity, which serves as the surrogate objective for improving the policy.
